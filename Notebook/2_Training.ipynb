{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "CNN Architecture - It is ResNet architecture, pre-trained network. It was pre-trained on millions of images, therefore it is already good at extracting low level features from RGB Images. Since our task is not to classsify, the final fully connected layers are removed, and replaced with our own fully connected layer to get a feature vector of size embed_size - to act as an interface for our RNN architecture. This last layer is subjected to training.\n",
    "\n",
    "RNN architecture - LSTM cell is used for generating text. Upsampling of the output of lstm to the vocab_size is done in 2 layer approach. Two layers of fully connected layers are used for proper upsampling. Drop out layer is also added to avoid overfitting.\n",
    "\n",
    "Embed_size = 256\n",
    "hiddden_size = 512\n",
    "The last upsampling of lstm outputs are done in 2 step approach as explained above -- (hidden_size) 512 -> 1024 -> 9955\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "Left it as default, as this itself is pretty good set of transforms for data augmentation.\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "CNN Architecture - Resnet pretrained weights are used and frozen, as it was trained on millions of RGB images, so it's good at extracting features from RGB images. Since last fully connected layer was newly added, it is trained, not frozen.\n",
    "\n",
    "RNN architecture - Since it is completely new, it was trained from scratch. Also it has 2 layer upsampling layers to upsample hidden size to the vocab size. (hidden_size) 512 -> 1024 -> 9955. Therefore, number of parameters is really high, it took around 12 hours for 3 epochs to train.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "Adam optimizer, since it is better than SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 457/414113 [00:00<01:30, 4569.31it/s]\u001b[A\n",
      "  0%|          | 865/414113 [00:00<01:33, 4408.96it/s]\u001b[A\n",
      "  0%|          | 1314/414113 [00:00<01:33, 4431.51it/s]\u001b[A\n",
      "  0%|          | 1776/414113 [00:00<01:31, 4484.49it/s]\u001b[A\n",
      "  1%|          | 2159/414113 [00:00<01:36, 4265.02it/s]\u001b[A\n",
      "  1%|          | 2605/414113 [00:00<01:35, 4320.99it/s]\u001b[A\n",
      "  1%|          | 3048/414113 [00:00<01:34, 4352.25it/s]\u001b[A\n",
      "  1%|          | 3499/414113 [00:00<01:33, 4396.50it/s]\u001b[A\n",
      "  1%|          | 3957/414113 [00:00<01:32, 4447.41it/s]\u001b[A\n",
      "  1%|          | 4416/414113 [00:01<01:31, 4488.78it/s]\u001b[A\n",
      "  1%|          | 4873/414113 [00:01<01:30, 4511.44it/s]\u001b[A\n",
      "  1%|▏         | 5335/414113 [00:01<01:30, 4541.51it/s]\u001b[A\n",
      "  1%|▏         | 5786/414113 [00:01<01:30, 4531.00it/s]\u001b[A\n",
      "  2%|▏         | 6240/414113 [00:01<01:29, 4533.04it/s]\u001b[A\n",
      "  2%|▏         | 6698/414113 [00:01<01:29, 4544.93it/s]\u001b[A\n",
      "  2%|▏         | 7151/414113 [00:01<01:29, 4534.88it/s]\u001b[A\n",
      "  2%|▏         | 7605/414113 [00:01<01:29, 4533.95it/s]\u001b[A\n",
      "  2%|▏         | 8067/414113 [00:01<01:29, 4559.10it/s]\u001b[A\n",
      "  2%|▏         | 8527/414113 [00:01<01:28, 4569.58it/s]\u001b[A\n",
      "  2%|▏         | 8984/414113 [00:02<01:28, 4554.03it/s]\u001b[A\n",
      "  2%|▏         | 9440/414113 [00:02<01:29, 4539.31it/s]\u001b[A\n",
      "  2%|▏         | 9894/414113 [00:02<01:29, 4523.93it/s]\u001b[A\n",
      "  2%|▏         | 10347/414113 [00:02<01:29, 4494.19it/s]\u001b[A\n",
      "  3%|▎         | 10797/414113 [00:02<01:29, 4495.41it/s]\u001b[A\n",
      "  3%|▎         | 11247/414113 [00:02<01:29, 4488.28it/s]\u001b[A\n",
      "  3%|▎         | 11697/414113 [00:02<01:29, 4490.93it/s]\u001b[A\n",
      "  3%|▎         | 12153/414113 [00:02<01:29, 4509.48it/s]\u001b[A\n",
      "  3%|▎         | 12611/414113 [00:02<01:28, 4530.00it/s]\u001b[A\n",
      "  3%|▎         | 13075/414113 [00:02<01:27, 4561.17it/s]\u001b[A\n",
      "  3%|▎         | 13538/414113 [00:03<01:27, 4580.16it/s]\u001b[A\n",
      "  3%|▎         | 14000/414113 [00:03<01:27, 4589.39it/s]\u001b[A\n",
      "  3%|▎         | 14461/414113 [00:03<01:26, 4594.93it/s]\u001b[A\n",
      "  4%|▎         | 14921/414113 [00:03<01:27, 4580.99it/s]\u001b[A\n",
      "  4%|▎         | 15389/414113 [00:03<01:26, 4607.79it/s]\u001b[A\n",
      "  4%|▍         | 15851/414113 [00:03<01:26, 4610.27it/s]\u001b[A\n",
      "  4%|▍         | 16313/414113 [00:03<01:26, 4579.27it/s]\u001b[A\n",
      "  4%|▍         | 16777/414113 [00:03<01:26, 4595.96it/s]\u001b[A\n",
      "  4%|▍         | 17237/414113 [00:03<01:26, 4594.43it/s]\u001b[A\n",
      "  4%|▍         | 17697/414113 [00:03<01:26, 4594.13it/s]\u001b[A\n",
      "  4%|▍         | 18160/414113 [00:04<01:25, 4604.58it/s]\u001b[A\n",
      "  4%|▍         | 18621/414113 [00:04<01:27, 4539.70it/s]\u001b[A\n",
      "  5%|▍         | 19076/414113 [00:04<01:27, 4516.13it/s]\u001b[A\n",
      "  5%|▍         | 19541/414113 [00:04<01:26, 4554.02it/s]\u001b[A\n",
      "  5%|▍         | 19997/414113 [00:04<01:26, 4549.68it/s]\u001b[A\n",
      "  5%|▍         | 20454/414113 [00:04<01:26, 4553.71it/s]\u001b[A\n",
      "  5%|▌         | 20910/414113 [00:04<01:26, 4543.39it/s]\u001b[A\n",
      "  5%|▌         | 21368/414113 [00:04<01:26, 4552.72it/s]\u001b[A\n",
      "  5%|▌         | 21824/414113 [00:04<01:31, 4293.33it/s]\u001b[A\n",
      "  5%|▌         | 22288/414113 [00:04<01:29, 4390.99it/s]\u001b[A\n",
      "  5%|▌         | 22757/414113 [00:05<01:27, 4476.17it/s]\u001b[A\n",
      "  6%|▌         | 23223/414113 [00:05<01:26, 4527.64it/s]\u001b[A\n",
      "  6%|▌         | 23678/414113 [00:05<01:26, 4534.01it/s]\u001b[A\n",
      "  6%|▌         | 24136/414113 [00:05<01:25, 4545.03it/s]\u001b[A\n",
      "  6%|▌         | 24592/414113 [00:05<01:26, 4505.24it/s]\u001b[A\n",
      "  6%|▌         | 25044/414113 [00:05<01:26, 4478.32it/s]\u001b[A\n",
      "  6%|▌         | 25506/414113 [00:05<01:25, 4518.78it/s]\u001b[A\n",
      "  6%|▋         | 25963/414113 [00:05<01:25, 4532.66it/s]\u001b[A\n",
      "  6%|▋         | 26417/414113 [00:05<01:25, 4528.19it/s]\u001b[A\n",
      "  6%|▋         | 26875/414113 [00:05<01:25, 4542.46it/s]\u001b[A\n",
      "  7%|▋         | 27330/414113 [00:06<01:25, 4534.44it/s]\u001b[A\n",
      "  7%|▋         | 27795/414113 [00:06<01:24, 4565.19it/s]\u001b[A\n",
      "  7%|▋         | 28254/414113 [00:06<01:24, 4569.71it/s]\u001b[A\n",
      "  7%|▋         | 28722/414113 [00:06<01:23, 4602.00it/s]\u001b[A\n",
      "  7%|▋         | 29183/414113 [00:06<01:24, 4578.09it/s]\u001b[A\n",
      "  7%|▋         | 29641/414113 [00:06<01:24, 4571.76it/s]\u001b[A\n",
      "  7%|▋         | 30104/414113 [00:06<01:23, 4587.97it/s]\u001b[A\n",
      "  7%|▋         | 30564/414113 [00:06<01:23, 4589.10it/s]\u001b[A\n",
      "  7%|▋         | 31039/414113 [00:06<01:22, 4633.75it/s]\u001b[A\n",
      "  8%|▊         | 31505/414113 [00:06<01:22, 4638.49it/s]\u001b[A\n",
      "  8%|▊         | 31971/414113 [00:07<01:22, 4642.61it/s]\u001b[A\n",
      "  8%|▊         | 32436/414113 [00:07<01:22, 4643.41it/s]\u001b[A\n",
      "  8%|▊         | 32901/414113 [00:07<01:23, 4591.96it/s]\u001b[A\n",
      "  8%|▊         | 33361/414113 [00:07<01:23, 4572.76it/s]\u001b[A\n",
      "  8%|▊         | 33819/414113 [00:07<01:23, 4558.48it/s]\u001b[A\n",
      "  8%|▊         | 34275/414113 [00:07<01:23, 4550.67it/s]\u001b[A\n",
      "  8%|▊         | 34731/414113 [00:07<01:23, 4531.46it/s]\u001b[A\n",
      "  8%|▊         | 35185/414113 [00:07<01:23, 4517.53it/s]\u001b[A\n",
      "  9%|▊         | 35637/414113 [00:07<01:24, 4484.99it/s]\u001b[A\n",
      "  9%|▊         | 36088/414113 [00:07<01:24, 4491.61it/s]\u001b[A\n",
      "  9%|▉         | 36551/414113 [00:08<01:23, 4529.96it/s]\u001b[A\n",
      "  9%|▉         | 37012/414113 [00:08<01:22, 4551.06it/s]\u001b[A\n",
      "  9%|▉         | 37468/414113 [00:08<01:22, 4541.19it/s]\u001b[A\n",
      "  9%|▉         | 37928/414113 [00:08<01:22, 4557.50it/s]\u001b[A\n",
      "  9%|▉         | 38389/414113 [00:08<01:22, 4571.44it/s]\u001b[A\n",
      "  9%|▉         | 38847/414113 [00:08<01:22, 4573.04it/s]\u001b[A\n",
      "  9%|▉         | 39305/414113 [00:08<01:22, 4555.75it/s]\u001b[A\n",
      " 10%|▉         | 39766/414113 [00:08<01:21, 4568.23it/s]\u001b[A\n",
      " 10%|▉         | 40223/414113 [00:08<01:22, 4535.68it/s]\u001b[A\n",
      " 10%|▉         | 40683/414113 [00:08<01:22, 4552.06it/s]\u001b[A\n",
      " 10%|▉         | 41142/414113 [00:09<01:21, 4561.02it/s]\u001b[A\n",
      " 10%|█         | 41601/414113 [00:09<01:21, 4569.65it/s]\u001b[A\n",
      " 10%|█         | 42059/414113 [00:09<01:21, 4543.91it/s]\u001b[A\n",
      " 10%|█         | 42515/414113 [00:09<01:21, 4545.94it/s]\u001b[A\n",
      " 10%|█         | 42970/414113 [00:09<01:21, 4529.19it/s]\u001b[A\n",
      " 10%|█         | 43428/414113 [00:09<01:21, 4544.05it/s]\u001b[A\n",
      " 11%|█         | 43883/414113 [00:09<01:22, 4509.66it/s]\u001b[A\n",
      " 11%|█         | 44335/414113 [00:10<02:19, 2646.10it/s]\u001b[A\n",
      " 11%|█         | 44790/414113 [00:10<02:02, 3025.43it/s]\u001b[A\n",
      " 11%|█         | 45253/414113 [00:10<01:49, 3375.22it/s]\u001b[A\n",
      " 11%|█         | 45706/414113 [00:10<01:40, 3653.88it/s]\u001b[A\n",
      " 11%|█         | 46166/414113 [00:10<01:34, 3893.68it/s]\u001b[A\n",
      " 11%|█▏        | 46622/414113 [00:10<01:30, 4070.22it/s]\u001b[A\n",
      " 11%|█▏        | 47067/414113 [00:10<01:27, 4176.16it/s]\u001b[A\n",
      " 11%|█▏        | 47509/414113 [00:10<01:26, 4224.05it/s]\u001b[A\n",
      " 12%|█▏        | 47955/414113 [00:10<01:25, 4289.37it/s]\u001b[A\n",
      " 12%|█▏        | 48409/414113 [00:10<01:23, 4358.65it/s]\u001b[A\n",
      " 12%|█▏        | 48854/414113 [00:11<01:23, 4385.34it/s]\u001b[A\n",
      " 12%|█▏        | 49311/414113 [00:11<01:22, 4437.18it/s]\u001b[A\n",
      " 12%|█▏        | 49770/414113 [00:11<01:21, 4481.50it/s]\u001b[A\n",
      " 12%|█▏        | 50222/414113 [00:11<01:26, 4203.47it/s]\u001b[A\n",
      " 12%|█▏        | 50684/414113 [00:11<01:24, 4319.41it/s]\u001b[A\n",
      " 12%|█▏        | 51135/414113 [00:11<01:22, 4373.84it/s]\u001b[A\n",
      " 12%|█▏        | 51596/414113 [00:11<01:21, 4442.06it/s]\u001b[A\n",
      " 13%|█▎        | 52056/414113 [00:11<01:20, 4486.22it/s]\u001b[A\n",
      " 13%|█▎        | 52526/414113 [00:11<01:19, 4529.20it/s]\u001b[A\n",
      " 13%|█▎        | 52981/414113 [00:11<01:19, 4515.53it/s]\u001b[A\n",
      " 13%|█▎        | 53438/414113 [00:12<01:19, 4530.02it/s]\u001b[A\n",
      " 13%|█▎        | 53892/414113 [00:12<01:19, 4525.98it/s]\u001b[A\n",
      " 13%|█▎        | 54349/414113 [00:12<01:19, 4538.81it/s]\u001b[A\n",
      " 13%|█▎        | 54808/414113 [00:12<01:18, 4552.16it/s]\u001b[A\n",
      " 13%|█▎        | 55268/414113 [00:12<01:18, 4563.50it/s]\u001b[A\n",
      " 13%|█▎        | 55729/414113 [00:12<01:18, 4574.91it/s]\u001b[A\n",
      " 14%|█▎        | 56196/414113 [00:12<01:17, 4600.91it/s]\u001b[A\n",
      " 14%|█▎        | 56660/414113 [00:12<01:17, 4603.39it/s]\u001b[A\n",
      " 14%|█▍        | 57127/414113 [00:12<01:17, 4620.32it/s]\u001b[A\n",
      " 14%|█▍        | 57590/414113 [00:12<01:17, 4598.35it/s]\u001b[A\n",
      " 14%|█▍        | 58050/414113 [00:13<01:17, 4598.27it/s]\u001b[A\n",
      " 14%|█▍        | 58512/414113 [00:13<01:17, 4602.93it/s]\u001b[A\n",
      " 14%|█▍        | 58973/414113 [00:13<01:18, 4530.33it/s]\u001b[A\n",
      " 14%|█▍        | 59436/414113 [00:13<01:17, 4559.73it/s]\u001b[A\n",
      " 14%|█▍        | 59898/414113 [00:13<01:17, 4575.09it/s]\u001b[A\n",
      " 15%|█▍        | 60356/414113 [00:13<01:17, 4568.18it/s]\u001b[A\n",
      " 15%|█▍        | 60813/414113 [00:13<01:17, 4561.97it/s]\u001b[A\n",
      " 15%|█▍        | 61272/414113 [00:13<01:17, 4567.70it/s]\u001b[A\n",
      " 15%|█▍        | 61729/414113 [00:13<01:17, 4565.22it/s]\u001b[A\n",
      " 15%|█▌        | 62186/414113 [00:13<01:17, 4543.40it/s]\u001b[A\n",
      " 15%|█▌        | 62641/414113 [00:14<01:17, 4519.97it/s]\u001b[A\n",
      " 15%|█▌        | 63094/414113 [00:14<01:17, 4520.81it/s]\u001b[A\n",
      " 15%|█▌        | 63547/414113 [00:14<01:17, 4511.99it/s]\u001b[A\n",
      " 15%|█▌        | 64011/414113 [00:14<01:16, 4547.37it/s]\u001b[A\n",
      " 16%|█▌        | 64466/414113 [00:14<01:17, 4496.65it/s]\u001b[A\n",
      " 16%|█▌        | 64916/414113 [00:14<01:19, 4412.42it/s]\u001b[A\n",
      " 16%|█▌        | 65373/414113 [00:14<01:18, 4458.47it/s]\u001b[A\n",
      " 16%|█▌        | 65828/414113 [00:14<01:17, 4483.90it/s]\u001b[A\n",
      " 16%|█▌        | 66287/414113 [00:14<01:17, 4513.28it/s]\u001b[A\n",
      " 16%|█▌        | 66739/414113 [00:14<01:17, 4498.91it/s]\u001b[A\n",
      " 16%|█▌        | 67193/414113 [00:15<01:16, 4508.63it/s]\u001b[A\n",
      " 16%|█▋        | 67653/414113 [00:15<01:16, 4533.77it/s]\u001b[A\n",
      " 16%|█▋        | 68107/414113 [00:15<01:16, 4526.09it/s]\u001b[A\n",
      " 17%|█▋        | 68565/414113 [00:15<01:16, 4541.04it/s]\u001b[A\n",
      " 17%|█▋        | 69021/414113 [00:15<01:15, 4546.65it/s]\u001b[A\n",
      " 17%|█▋        | 69476/414113 [00:15<01:16, 4525.36it/s]\u001b[A\n",
      " 17%|█▋        | 69937/414113 [00:15<01:15, 4549.55it/s]\u001b[A\n",
      " 17%|█▋        | 70399/414113 [00:15<01:15, 4569.32it/s]\u001b[A\n",
      " 17%|█▋        | 70857/414113 [00:15<01:15, 4534.57it/s]\u001b[A\n",
      " 17%|█▋        | 71314/414113 [00:15<01:15, 4543.20it/s]\u001b[A\n",
      " 17%|█▋        | 71769/414113 [00:16<01:17, 4401.88it/s]\u001b[A\n",
      " 17%|█▋        | 72222/414113 [00:16<01:17, 4438.75it/s]\u001b[A\n",
      " 18%|█▊        | 72676/414113 [00:16<01:16, 4467.85it/s]\u001b[A\n",
      " 18%|█▊        | 73133/414113 [00:16<01:15, 4496.64it/s]\u001b[A\n",
      " 18%|█▊        | 73596/414113 [00:16<01:15, 4533.32it/s]\u001b[A\n",
      " 18%|█▊        | 74050/414113 [00:16<01:15, 4499.09it/s]\u001b[A\n",
      " 18%|█▊        | 74501/414113 [00:16<01:15, 4479.39it/s]\u001b[A\n",
      " 18%|█▊        | 74956/414113 [00:16<01:15, 4500.18it/s]\u001b[A\n",
      " 18%|█▊        | 75410/414113 [00:16<01:15, 4509.75it/s]\u001b[A\n",
      " 18%|█▊        | 75862/414113 [00:16<01:15, 4502.03it/s]\u001b[A\n",
      " 18%|█▊        | 76317/414113 [00:17<01:14, 4515.46it/s]\u001b[A\n",
      " 19%|█▊        | 76771/414113 [00:17<01:14, 4520.62it/s]\u001b[A\n",
      " 19%|█▊        | 77227/414113 [00:17<01:14, 4530.76it/s]\u001b[A\n",
      " 19%|█▉        | 77682/414113 [00:17<01:14, 4534.91it/s]\u001b[A\n",
      " 19%|█▉        | 78142/414113 [00:17<01:13, 4552.36it/s]\u001b[A\n",
      " 19%|█▉        | 78598/414113 [00:17<01:13, 4545.25it/s]\u001b[A\n",
      " 19%|█▉        | 79062/414113 [00:17<01:13, 4571.86it/s]\u001b[A\n",
      " 19%|█▉        | 79520/414113 [00:17<01:13, 4557.85it/s]\u001b[A\n",
      " 19%|█▉        | 79983/414113 [00:17<01:12, 4578.09it/s]\u001b[A\n",
      " 19%|█▉        | 80446/414113 [00:17<01:12, 4592.43it/s]\u001b[A\n",
      " 20%|█▉        | 80910/414113 [00:18<01:12, 4605.02it/s]\u001b[A\n",
      " 20%|█▉        | 81375/414113 [00:18<01:12, 4617.78it/s]\u001b[A\n",
      " 20%|█▉        | 81837/414113 [00:18<01:12, 4606.90it/s]\u001b[A\n",
      " 20%|█▉        | 82308/414113 [00:18<01:11, 4637.23it/s]\u001b[A\n",
      " 20%|█▉        | 82772/414113 [00:18<01:12, 4581.39it/s]\u001b[A\n",
      " 20%|██        | 83231/414113 [00:18<01:12, 4569.18it/s]\u001b[A\n",
      " 20%|██        | 83692/414113 [00:18<01:12, 4579.86it/s]\u001b[A\n",
      " 20%|██        | 84153/414113 [00:18<01:11, 4585.61it/s]\u001b[A\n",
      " 20%|██        | 84612/414113 [00:18<01:12, 4543.03it/s]\u001b[A\n",
      " 21%|██        | 85067/414113 [00:19<01:12, 4524.73it/s]\u001b[A\n",
      " 21%|██        | 85522/414113 [00:19<01:12, 4530.79it/s]\u001b[A\n",
      " 21%|██        | 85978/414113 [00:19<01:12, 4537.16it/s]\u001b[A\n",
      " 21%|██        | 86445/414113 [00:19<01:11, 4573.71it/s]\u001b[A\n",
      " 21%|██        | 86912/414113 [00:19<01:11, 4600.61it/s]\u001b[A\n",
      " 21%|██        | 87373/414113 [00:19<01:11, 4600.84it/s]\u001b[A\n",
      " 21%|██        | 87834/414113 [00:19<01:11, 4579.56it/s]\u001b[A\n",
      " 21%|██▏       | 88295/414113 [00:19<01:11, 4586.33it/s]\u001b[A\n",
      " 21%|██▏       | 88758/414113 [00:19<01:10, 4597.94it/s]\u001b[A\n",
      " 22%|██▏       | 89218/414113 [00:19<01:11, 4564.32it/s]\u001b[A\n",
      " 22%|██▏       | 89677/414113 [00:20<01:10, 4571.44it/s]\u001b[A\n",
      " 22%|██▏       | 90135/414113 [00:20<01:11, 4543.59it/s]\u001b[A\n",
      " 22%|██▏       | 90590/414113 [00:20<01:12, 4486.59it/s]\u001b[A\n",
      " 22%|██▏       | 91050/414113 [00:20<01:11, 4517.91it/s]\u001b[A\n",
      " 22%|██▏       | 91502/414113 [00:20<01:11, 4492.95it/s]\u001b[A\n",
      " 22%|██▏       | 91961/414113 [00:20<01:11, 4519.51it/s]\u001b[A\n",
      " 22%|██▏       | 92414/414113 [00:20<01:11, 4507.34it/s]\u001b[A\n",
      " 22%|██▏       | 92873/414113 [00:20<01:10, 4531.00it/s]\u001b[A\n",
      " 23%|██▎       | 93327/414113 [00:20<01:11, 4476.81it/s]\u001b[A\n",
      " 23%|██▎       | 93775/414113 [00:20<01:11, 4475.07it/s]\u001b[A\n",
      " 23%|██▎       | 94232/414113 [00:21<01:11, 4501.58it/s]\u001b[A\n",
      " 23%|██▎       | 94683/414113 [00:21<01:11, 4494.00it/s]\u001b[A\n",
      " 23%|██▎       | 95143/414113 [00:21<01:10, 4523.72it/s]\u001b[A\n",
      " 23%|██▎       | 95604/414113 [00:21<01:10, 4546.74it/s]\u001b[A\n",
      " 23%|██▎       | 96069/414113 [00:21<01:09, 4574.91it/s]\u001b[A\n",
      " 23%|██▎       | 96534/414113 [00:21<01:09, 4596.67it/s]\u001b[A\n",
      " 23%|██▎       | 96994/414113 [00:21<01:12, 4366.16it/s]\u001b[A\n",
      " 24%|██▎       | 97456/414113 [00:21<01:11, 4437.55it/s]\u001b[A\n",
      " 24%|██▎       | 97905/414113 [00:21<01:11, 4452.60it/s]\u001b[A\n",
      " 24%|██▍       | 98352/414113 [00:21<01:11, 4410.07it/s]\u001b[A\n",
      " 24%|██▍       | 98803/414113 [00:22<01:11, 4437.71it/s]\u001b[A\n",
      " 24%|██▍       | 99249/414113 [00:22<01:10, 4442.10it/s]\u001b[A\n",
      " 24%|██▍       | 99704/414113 [00:22<01:10, 4473.31it/s]\u001b[A\n",
      " 24%|██▍       | 100152/414113 [00:22<01:10, 4437.47it/s]\u001b[A\n",
      " 24%|██▍       | 100598/414113 [00:22<01:10, 4444.18it/s]\u001b[A\n",
      " 24%|██▍       | 101043/414113 [00:22<01:10, 4413.37it/s]\u001b[A\n",
      " 25%|██▍       | 101487/414113 [00:22<01:10, 4419.12it/s]\u001b[A\n",
      " 25%|██▍       | 101930/414113 [00:22<01:10, 4418.42it/s]\u001b[A\n",
      " 25%|██▍       | 102392/414113 [00:22<01:09, 4477.02it/s]\u001b[A\n",
      " 25%|██▍       | 102840/414113 [00:22<01:09, 4475.22it/s]\u001b[A\n",
      " 25%|██▍       | 103298/414113 [00:23<01:09, 4504.40it/s]\u001b[A\n",
      " 25%|██▌       | 103758/414113 [00:23<01:08, 4531.66it/s]\u001b[A\n",
      " 25%|██▌       | 104212/414113 [00:23<01:08, 4527.60it/s]\u001b[A\n",
      " 25%|██▌       | 104665/414113 [00:23<01:09, 4457.75it/s]\u001b[A\n",
      " 25%|██▌       | 105112/414113 [00:23<01:10, 4366.34it/s]\u001b[A\n",
      " 25%|██▌       | 105550/414113 [00:23<01:10, 4355.38it/s]\u001b[A\n",
      " 26%|██▌       | 105986/414113 [00:23<01:11, 4309.83it/s]\u001b[A\n",
      " 26%|██▌       | 106439/414113 [00:23<01:10, 4371.87it/s]\u001b[A\n",
      " 26%|██▌       | 106881/414113 [00:23<01:10, 4385.10it/s]\u001b[A\n",
      " 26%|██▌       | 107338/414113 [00:23<01:09, 4437.04it/s]\u001b[A\n",
      " 26%|██▌       | 107789/414113 [00:24<01:08, 4457.87it/s]\u001b[A\n",
      " 26%|██▌       | 108254/414113 [00:24<01:07, 4510.89it/s]\u001b[A\n",
      " 26%|██▋       | 108709/414113 [00:24<01:07, 4520.61it/s]\u001b[A\n",
      " 26%|██▋       | 109171/414113 [00:24<01:07, 4549.95it/s]\u001b[A\n",
      " 26%|██▋       | 109627/414113 [00:24<01:07, 4503.70it/s]\u001b[A\n",
      " 27%|██▋       | 110086/414113 [00:24<01:07, 4528.09it/s]\u001b[A\n",
      " 27%|██▋       | 110540/414113 [00:24<01:07, 4526.17it/s]\u001b[A\n",
      " 27%|██▋       | 110993/414113 [00:24<01:07, 4513.35it/s]\u001b[A\n",
      " 27%|██▋       | 111456/414113 [00:24<01:06, 4546.43it/s]\u001b[A\n",
      " 27%|██▋       | 111922/414113 [00:24<01:05, 4579.87it/s]\u001b[A\n",
      " 27%|██▋       | 112381/414113 [00:25<01:06, 4542.22it/s]\u001b[A\n",
      " 27%|██▋       | 112841/414113 [00:25<01:06, 4558.86it/s]\u001b[A\n",
      " 27%|██▋       | 113298/414113 [00:25<01:06, 4546.50it/s]\u001b[A\n",
      " 27%|██▋       | 113767/414113 [00:25<01:05, 4586.30it/s]\u001b[A\n",
      " 28%|██▊       | 114226/414113 [00:25<01:06, 4533.50it/s]\u001b[A\n",
      " 28%|██▊       | 114680/414113 [00:25<01:06, 4475.93it/s]\u001b[A\n",
      " 28%|██▊       | 115137/414113 [00:25<01:06, 4501.46it/s]\u001b[A\n",
      " 28%|██▊       | 115602/414113 [00:25<01:05, 4544.85it/s]\u001b[A\n",
      " 28%|██▊       | 116057/414113 [00:25<01:05, 4544.98it/s]\u001b[A\n",
      " 28%|██▊       | 116527/414113 [00:25<01:04, 4588.79it/s]\u001b[A\n",
      " 28%|██▊       | 116987/414113 [00:26<01:04, 4579.43it/s]\u001b[A\n",
      " 28%|██▊       | 117451/414113 [00:26<01:04, 4595.83it/s]\u001b[A\n",
      " 28%|██▊       | 117911/414113 [00:26<01:07, 4409.88it/s]\u001b[A\n",
      " 29%|██▊       | 118354/414113 [00:26<01:07, 4411.56it/s]\u001b[A\n",
      " 29%|██▊       | 118797/414113 [00:26<01:07, 4356.75it/s]\u001b[A\n",
      " 29%|██▉       | 119237/414113 [00:26<01:07, 4368.47it/s]\u001b[A\n",
      " 29%|██▉       | 119675/414113 [00:26<01:08, 4328.96it/s]\u001b[A\n",
      " 29%|██▉       | 120122/414113 [00:26<01:07, 4368.66it/s]\u001b[A\n",
      " 29%|██▉       | 120575/414113 [00:26<01:06, 4415.33it/s]\u001b[A\n",
      " 29%|██▉       | 121036/414113 [00:27<01:05, 4470.66it/s]\u001b[A\n",
      " 29%|██▉       | 121492/414113 [00:27<01:05, 4496.38it/s]\u001b[A\n",
      " 29%|██▉       | 121943/414113 [00:27<01:05, 4482.76it/s]\u001b[A\n",
      " 30%|██▉       | 122396/414113 [00:27<01:04, 4495.61it/s]\u001b[A\n",
      " 30%|██▉       | 122852/414113 [00:27<01:04, 4514.03it/s]\u001b[A\n",
      " 30%|██▉       | 123304/414113 [00:27<01:04, 4485.90it/s]\u001b[A\n",
      " 30%|██▉       | 123753/414113 [00:27<01:05, 4465.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 124209/414113 [00:27<01:04, 4492.45it/s]\u001b[A\n",
      " 30%|███       | 124663/414113 [00:27<01:04, 4503.76it/s]\u001b[A\n",
      " 30%|███       | 125132/414113 [00:27<01:03, 4557.17it/s]\u001b[A\n",
      " 30%|███       | 125588/414113 [00:28<01:03, 4545.72it/s]\u001b[A\n",
      " 30%|███       | 126044/414113 [00:28<01:03, 4547.39it/s]\u001b[A\n",
      " 31%|███       | 126504/414113 [00:28<01:03, 4562.74it/s]\u001b[A\n",
      " 31%|███       | 126961/414113 [00:28<01:02, 4559.40it/s]\u001b[A\n",
      " 31%|███       | 127419/414113 [00:28<01:02, 4562.63it/s]\u001b[A\n",
      " 31%|███       | 127876/414113 [00:28<01:03, 4485.70it/s]\u001b[A\n",
      " 31%|███       | 128325/414113 [00:28<01:03, 4480.90it/s]\u001b[A\n",
      " 31%|███       | 128782/414113 [00:28<01:03, 4505.26it/s]\u001b[A\n",
      " 31%|███       | 129235/414113 [00:28<01:03, 4510.03it/s]\u001b[A\n",
      " 31%|███▏      | 129700/414113 [00:28<01:02, 4551.08it/s]\u001b[A\n",
      " 31%|███▏      | 130156/414113 [00:29<01:02, 4550.26it/s]\u001b[A\n",
      " 32%|███▏      | 130619/414113 [00:29<01:02, 4571.86it/s]\u001b[A\n",
      " 32%|███▏      | 131077/414113 [00:29<01:02, 4517.16it/s]\u001b[A\n",
      " 32%|███▏      | 131537/414113 [00:29<01:02, 4541.01it/s]\u001b[A\n",
      " 32%|███▏      | 131992/414113 [00:29<01:02, 4530.76it/s]\u001b[A\n",
      " 32%|███▏      | 132452/414113 [00:29<01:01, 4548.57it/s]\u001b[A\n",
      " 32%|███▏      | 132907/414113 [00:29<01:02, 4533.77it/s]\u001b[A\n",
      " 32%|███▏      | 133364/414113 [00:29<01:01, 4542.58it/s]\u001b[A\n",
      " 32%|███▏      | 133819/414113 [00:29<01:01, 4534.94it/s]\u001b[A\n",
      " 32%|███▏      | 134273/414113 [00:29<01:02, 4445.32it/s]\u001b[A\n",
      " 33%|███▎      | 134736/414113 [00:30<01:02, 4496.31it/s]\u001b[A\n",
      " 33%|███▎      | 135192/414113 [00:30<01:01, 4514.00it/s]\u001b[A\n",
      " 33%|███▎      | 135665/414113 [00:30<01:00, 4575.64it/s]\u001b[A\n",
      " 33%|███▎      | 136123/414113 [00:30<01:01, 4527.29it/s]\u001b[A\n",
      " 33%|███▎      | 136582/414113 [00:30<01:01, 4544.93it/s]\u001b[A\n",
      " 33%|███▎      | 137037/414113 [00:30<01:01, 4495.40it/s]\u001b[A\n",
      " 33%|███▎      | 137487/414113 [00:30<01:01, 4489.01it/s]\u001b[A\n",
      " 33%|███▎      | 137944/414113 [00:30<01:01, 4512.83it/s]\u001b[A\n",
      " 33%|███▎      | 138404/414113 [00:30<01:00, 4536.72it/s]\u001b[A\n",
      " 34%|███▎      | 138858/414113 [00:30<01:01, 4474.66it/s]\u001b[A\n",
      " 34%|███▎      | 139318/414113 [00:31<01:00, 4510.61it/s]\u001b[A\n",
      " 34%|███▍      | 139773/414113 [00:31<01:00, 4522.19it/s]\u001b[A\n",
      " 34%|███▍      | 140226/414113 [00:31<01:02, 4356.59it/s]\u001b[A\n",
      " 34%|███▍      | 140681/414113 [00:31<01:01, 4412.07it/s]\u001b[A\n",
      " 34%|███▍      | 141133/414113 [00:31<01:01, 4442.95it/s]\u001b[A\n",
      " 34%|███▍      | 141579/414113 [00:31<01:01, 4398.56it/s]\u001b[A\n",
      " 34%|███▍      | 142023/414113 [00:31<01:01, 4409.86it/s]\u001b[A\n",
      " 34%|███▍      | 142475/414113 [00:31<01:01, 4441.00it/s]\u001b[A\n",
      " 35%|███▍      | 142930/414113 [00:31<01:00, 4471.15it/s]\u001b[A\n",
      " 35%|███▍      | 143386/414113 [00:31<01:00, 4496.01it/s]\u001b[A\n",
      " 35%|███▍      | 143840/414113 [00:32<00:59, 4509.03it/s]\u001b[A\n",
      " 35%|███▍      | 144292/414113 [00:32<01:00, 4492.86it/s]\u001b[A\n",
      " 35%|███▍      | 144748/414113 [00:32<00:59, 4511.83it/s]\u001b[A\n",
      " 35%|███▌      | 145214/414113 [00:32<00:59, 4554.49it/s]\u001b[A\n",
      " 35%|███▌      | 145670/414113 [00:32<00:59, 4548.86it/s]\u001b[A\n",
      " 35%|███▌      | 146126/414113 [00:32<00:59, 4537.85it/s]\u001b[A\n",
      " 35%|███▌      | 146589/414113 [00:32<00:58, 4562.70it/s]\u001b[A\n",
      " 36%|███▌      | 147058/414113 [00:32<00:58, 4600.13it/s]\u001b[A\n",
      " 36%|███▌      | 147519/414113 [00:32<00:58, 4572.52it/s]\u001b[A\n",
      " 36%|███▌      | 147978/414113 [00:32<00:58, 4576.23it/s]\u001b[A\n",
      " 36%|███▌      | 148436/414113 [00:33<00:58, 4570.26it/s]\u001b[A\n",
      " 36%|███▌      | 148894/414113 [00:33<00:58, 4530.27it/s]\u001b[A\n",
      " 36%|███▌      | 149356/414113 [00:33<00:58, 4554.23it/s]\u001b[A\n",
      " 36%|███▌      | 149813/414113 [00:33<00:57, 4558.92it/s]\u001b[A\n",
      " 36%|███▋      | 150269/414113 [00:33<00:58, 4548.43it/s]\u001b[A\n",
      " 36%|███▋      | 150732/414113 [00:33<00:57, 4571.46it/s]\u001b[A\n",
      " 37%|███▋      | 151192/414113 [00:33<00:57, 4579.02it/s]\u001b[A\n",
      " 37%|███▋      | 151659/414113 [00:33<00:57, 4602.89it/s]\u001b[A\n",
      " 37%|███▋      | 152120/414113 [00:33<00:56, 4597.65it/s]\u001b[A\n",
      " 37%|███▋      | 152580/414113 [00:33<00:56, 4589.80it/s]\u001b[A\n",
      " 37%|███▋      | 153051/414113 [00:34<00:56, 4623.15it/s]\u001b[A\n",
      " 37%|███▋      | 153514/414113 [00:34<00:56, 4619.26it/s]\u001b[A\n",
      " 37%|███▋      | 153981/414113 [00:34<00:56, 4633.59it/s]\u001b[A\n",
      " 37%|███▋      | 154445/414113 [00:34<00:56, 4627.33it/s]\u001b[A\n",
      " 37%|███▋      | 154908/414113 [00:34<00:56, 4588.15it/s]\u001b[A\n",
      " 38%|███▊      | 155367/414113 [00:34<00:56, 4549.93it/s]\u001b[A\n",
      " 38%|███▊      | 155823/414113 [00:34<00:58, 4415.42it/s]\u001b[A\n",
      " 38%|███▊      | 156290/414113 [00:34<00:57, 4487.92it/s]\u001b[A\n",
      " 38%|███▊      | 156766/414113 [00:34<00:56, 4565.49it/s]\u001b[A\n",
      " 38%|███▊      | 157225/414113 [00:35<00:56, 4570.02it/s]\u001b[A\n",
      " 38%|███▊      | 157686/414113 [00:35<00:55, 4579.22it/s]\u001b[A\n",
      " 38%|███▊      | 158163/414113 [00:35<00:55, 4633.45it/s]\u001b[A\n",
      " 38%|███▊      | 158627/414113 [00:35<00:55, 4629.89it/s]\u001b[A\n",
      " 38%|███▊      | 159091/414113 [00:35<00:55, 4625.60it/s]\u001b[A\n",
      " 39%|███▊      | 159554/414113 [00:35<00:55, 4602.81it/s]\u001b[A\n",
      " 39%|███▊      | 160015/414113 [00:35<00:55, 4600.49it/s]\u001b[A\n",
      " 39%|███▉      | 160480/414113 [00:35<00:54, 4614.36it/s]\u001b[A\n",
      " 39%|███▉      | 160942/414113 [00:35<00:54, 4612.74it/s]\u001b[A\n",
      " 39%|███▉      | 161411/414113 [00:35<00:54, 4635.43it/s]\u001b[A\n",
      " 39%|███▉      | 161875/414113 [00:36<01:39, 2524.19it/s]\u001b[A\n",
      " 39%|███▉      | 162345/414113 [00:36<01:25, 2931.00it/s]\u001b[A\n",
      " 39%|███▉      | 162806/414113 [00:36<01:16, 3290.53it/s]\u001b[A\n",
      " 39%|███▉      | 163273/414113 [00:36<01:09, 3608.66it/s]\u001b[A\n",
      " 40%|███▉      | 163739/414113 [00:36<01:04, 3870.52it/s]\u001b[A\n",
      " 40%|███▉      | 164202/414113 [00:36<01:01, 4070.37it/s]\u001b[A\n",
      " 40%|███▉      | 164654/414113 [00:36<00:59, 4193.90it/s]\u001b[A\n",
      " 40%|███▉      | 165116/414113 [00:36<00:57, 4311.67it/s]\u001b[A\n",
      " 40%|███▉      | 165569/414113 [00:37<00:56, 4373.81it/s]\u001b[A\n",
      " 40%|████      | 166022/414113 [00:37<00:56, 4409.55it/s]\u001b[A\n",
      " 40%|████      | 166474/414113 [00:37<00:56, 4405.50it/s]\u001b[A\n",
      " 40%|████      | 166940/414113 [00:37<00:55, 4474.65it/s]\u001b[A\n",
      " 40%|████      | 167393/414113 [00:37<00:55, 4476.17it/s]\u001b[A\n",
      " 41%|████      | 167845/414113 [00:37<00:54, 4484.20it/s]\u001b[A\n",
      " 41%|████      | 168314/414113 [00:37<00:54, 4541.79it/s]\u001b[A\n",
      " 41%|████      | 168773/414113 [00:37<00:53, 4555.94it/s]\u001b[A\n",
      " 41%|████      | 169231/414113 [00:37<00:53, 4545.18it/s]\u001b[A\n",
      " 41%|████      | 169687/414113 [00:38<00:54, 4489.95it/s]\u001b[A\n",
      " 41%|████      | 170137/414113 [00:38<00:54, 4492.90it/s]\u001b[A\n",
      " 41%|████      | 170603/414113 [00:38<00:53, 4541.70it/s]\u001b[A\n",
      " 41%|████▏     | 171065/414113 [00:38<00:53, 4563.45it/s]\u001b[A\n",
      " 41%|████▏     | 171522/414113 [00:38<00:53, 4552.77it/s]\u001b[A\n",
      " 42%|████▏     | 171979/414113 [00:38<00:53, 4557.63it/s]\u001b[A\n",
      " 42%|████▏     | 172435/414113 [00:38<00:53, 4520.64it/s]\u001b[A\n",
      " 42%|████▏     | 172888/414113 [00:38<00:53, 4510.90it/s]\u001b[A\n",
      " 42%|████▏     | 173340/414113 [00:38<00:54, 4431.28it/s]\u001b[A\n",
      " 42%|████▏     | 173784/414113 [00:38<00:54, 4433.89it/s]\u001b[A\n",
      " 42%|████▏     | 174228/414113 [00:39<00:54, 4420.38it/s]\u001b[A\n",
      " 42%|████▏     | 174671/414113 [00:39<00:55, 4294.82it/s]\u001b[A\n",
      " 42%|████▏     | 175113/414113 [00:39<00:55, 4329.32it/s]\u001b[A\n",
      " 42%|████▏     | 175552/414113 [00:39<00:54, 4345.26it/s]\u001b[A\n",
      " 43%|████▎     | 176006/414113 [00:39<00:54, 4400.01it/s]\u001b[A\n",
      " 43%|████▎     | 176447/414113 [00:39<00:54, 4371.07it/s]\u001b[A\n",
      " 43%|████▎     | 176905/414113 [00:39<00:53, 4430.00it/s]\u001b[A\n",
      " 43%|████▎     | 177364/414113 [00:39<00:52, 4473.72it/s]\u001b[A\n",
      " 43%|████▎     | 177826/414113 [00:39<00:52, 4514.39it/s]\u001b[A\n",
      " 43%|████▎     | 178278/414113 [00:39<00:52, 4490.28it/s]\u001b[A\n",
      " 43%|████▎     | 178741/414113 [00:40<00:51, 4529.09it/s]\u001b[A\n",
      " 43%|████▎     | 179198/414113 [00:40<00:51, 4540.76it/s]\u001b[A\n",
      " 43%|████▎     | 179658/414113 [00:40<00:51, 4558.28it/s]\u001b[A\n",
      " 43%|████▎     | 180114/414113 [00:40<00:51, 4542.87it/s]\u001b[A\n",
      " 44%|████▎     | 180569/414113 [00:40<00:51, 4519.19it/s]\u001b[A\n",
      " 44%|████▎     | 181023/414113 [00:40<00:51, 4525.25it/s]\u001b[A\n",
      " 44%|████▍     | 181487/414113 [00:40<00:51, 4557.83it/s]\u001b[A\n",
      " 44%|████▍     | 181946/414113 [00:40<00:50, 4566.46it/s]\u001b[A\n",
      " 44%|████▍     | 182404/414113 [00:40<00:50, 4570.27it/s]\u001b[A\n",
      " 44%|████▍     | 182862/414113 [00:40<00:50, 4549.81it/s]\u001b[A\n",
      " 44%|████▍     | 183318/414113 [00:41<00:50, 4531.35it/s]\u001b[A\n",
      " 44%|████▍     | 183775/414113 [00:41<00:50, 4540.23it/s]\u001b[A\n",
      " 44%|████▍     | 184236/414113 [00:41<00:50, 4560.51it/s]\u001b[A\n",
      " 45%|████▍     | 184696/414113 [00:41<00:50, 4569.90it/s]\u001b[A\n",
      " 45%|████▍     | 185154/414113 [00:41<00:50, 4571.10it/s]\u001b[A\n",
      " 45%|████▍     | 185612/414113 [00:41<00:50, 4563.30it/s]\u001b[A\n",
      " 45%|████▍     | 186069/414113 [00:41<00:50, 4538.41it/s]\u001b[A\n",
      " 45%|████▌     | 186523/414113 [00:41<00:50, 4531.16it/s]\u001b[A\n",
      " 45%|████▌     | 186988/414113 [00:41<00:49, 4566.00it/s]\u001b[A\n",
      " 45%|████▌     | 187449/414113 [00:41<00:49, 4577.67it/s]\u001b[A\n",
      " 45%|████▌     | 187908/414113 [00:42<00:49, 4578.51it/s]\u001b[A\n",
      " 45%|████▌     | 188366/414113 [00:42<00:49, 4566.11it/s]\u001b[A\n",
      " 46%|████▌     | 188823/414113 [00:42<00:49, 4553.87it/s]\u001b[A\n",
      " 46%|████▌     | 189288/414113 [00:42<00:49, 4579.98it/s]\u001b[A\n",
      " 46%|████▌     | 189747/414113 [00:42<00:49, 4576.47it/s]\u001b[A\n",
      " 46%|████▌     | 190205/414113 [00:42<00:48, 4571.20it/s]\u001b[A\n",
      " 46%|████▌     | 190677/414113 [00:42<00:48, 4614.61it/s]\u001b[A\n",
      " 46%|████▌     | 191143/414113 [00:42<00:48, 4625.70it/s]\u001b[A\n",
      " 46%|████▋     | 191613/414113 [00:42<00:47, 4647.55it/s]\u001b[A\n",
      " 46%|████▋     | 192078/414113 [00:42<00:48, 4617.05it/s]\u001b[A\n",
      " 46%|████▋     | 192544/414113 [00:43<00:47, 4627.60it/s]\u001b[A\n",
      " 47%|████▋     | 193007/414113 [00:43<00:47, 4610.65it/s]\u001b[A\n",
      " 47%|████▋     | 193469/414113 [00:43<00:48, 4586.82it/s]\u001b[A\n",
      " 47%|████▋     | 193938/414113 [00:43<00:47, 4612.58it/s]\u001b[A\n",
      " 47%|████▋     | 194400/414113 [00:43<00:47, 4611.64it/s]\u001b[A\n",
      " 47%|████▋     | 194862/414113 [00:43<00:48, 4563.27it/s]\u001b[A\n",
      " 47%|████▋     | 195323/414113 [00:43<00:47, 4575.75it/s]\u001b[A\n",
      " 47%|████▋     | 195785/414113 [00:43<00:47, 4586.49it/s]\u001b[A\n",
      " 47%|████▋     | 196245/414113 [00:43<00:47, 4589.58it/s]\u001b[A\n",
      " 48%|████▊     | 196705/414113 [00:43<00:47, 4568.46it/s]\u001b[A\n",
      " 48%|████▊     | 197162/414113 [00:44<00:47, 4562.77it/s]\u001b[A\n",
      " 48%|████▊     | 197626/414113 [00:44<00:47, 4583.09it/s]\u001b[A\n",
      " 48%|████▊     | 198092/414113 [00:44<00:46, 4603.57it/s]\u001b[A\n",
      " 48%|████▊     | 198553/414113 [00:44<00:46, 4600.54it/s]\u001b[A\n",
      " 48%|████▊     | 199016/414113 [00:44<00:46, 4609.03it/s]\u001b[A\n",
      " 48%|████▊     | 199477/414113 [00:44<00:47, 4526.56it/s]\u001b[A\n",
      " 48%|████▊     | 199931/414113 [00:44<00:47, 4518.73it/s]\u001b[A\n",
      " 48%|████▊     | 200391/414113 [00:44<00:47, 4541.23it/s]\u001b[A\n",
      " 49%|████▊     | 200846/414113 [00:44<00:47, 4521.07it/s]\u001b[A\n",
      " 49%|████▊     | 201299/414113 [00:44<00:47, 4512.81it/s]\u001b[A\n",
      " 49%|████▊     | 201751/414113 [00:45<00:47, 4486.29it/s]\u001b[A\n",
      " 49%|████▉     | 202201/414113 [00:45<00:47, 4487.64it/s]\u001b[A\n",
      " 49%|████▉     | 202652/414113 [00:45<00:47, 4492.53it/s]\u001b[A\n",
      " 49%|████▉     | 203104/414113 [00:45<00:46, 4500.59it/s]\u001b[A\n",
      " 49%|████▉     | 203563/414113 [00:45<00:46, 4525.82it/s]\u001b[A\n",
      " 49%|████▉     | 204020/414113 [00:45<00:46, 4537.69it/s]\u001b[A\n",
      " 49%|████▉     | 204478/414113 [00:45<00:46, 4549.05it/s]\u001b[A\n",
      " 49%|████▉     | 204938/414113 [00:45<00:45, 4563.72it/s]\u001b[A\n",
      " 50%|████▉     | 205397/414113 [00:45<00:45, 4569.16it/s]\u001b[A\n",
      " 50%|████▉     | 205854/414113 [00:45<00:45, 4557.74it/s]\u001b[A\n",
      " 50%|████▉     | 206310/414113 [00:46<00:45, 4553.27it/s]\u001b[A\n",
      " 50%|████▉     | 206766/414113 [00:46<00:45, 4550.75it/s]\u001b[A\n",
      " 50%|█████     | 207222/414113 [00:46<00:45, 4552.77it/s]\u001b[A\n",
      " 50%|█████     | 207680/414113 [00:46<00:45, 4558.04it/s]\u001b[A\n",
      " 50%|█████     | 208136/414113 [00:46<00:45, 4538.31it/s]\u001b[A\n",
      " 50%|█████     | 208590/414113 [00:46<00:45, 4517.14it/s]\u001b[A\n",
      " 50%|█████     | 209055/414113 [00:46<00:45, 4555.18it/s]\u001b[A\n",
      " 51%|█████     | 209523/414113 [00:46<00:44, 4590.72it/s]\u001b[A\n",
      " 51%|█████     | 209989/414113 [00:46<00:44, 4610.71it/s]\u001b[A\n",
      " 51%|█████     | 210462/414113 [00:46<00:43, 4644.32it/s]\u001b[A\n",
      " 51%|█████     | 210927/414113 [00:47<00:43, 4629.33it/s]\u001b[A\n",
      " 51%|█████     | 211391/414113 [00:47<00:44, 4597.23it/s]\u001b[A\n",
      " 51%|█████     | 211851/414113 [00:47<00:44, 4584.07it/s]\u001b[A\n",
      " 51%|█████▏    | 212314/414113 [00:47<00:43, 4596.74it/s]\u001b[A\n",
      " 51%|█████▏    | 212774/414113 [00:47<00:43, 4594.27it/s]\u001b[A\n",
      " 51%|█████▏    | 213234/414113 [00:47<00:43, 4595.35it/s]\u001b[A\n",
      " 52%|█████▏    | 213704/414113 [00:47<00:43, 4623.86it/s]\u001b[A\n",
      " 52%|█████▏    | 214170/414113 [00:47<00:43, 4632.27it/s]\u001b[A\n",
      " 52%|█████▏    | 214639/414113 [00:47<00:42, 4647.62it/s]\u001b[A\n",
      " 52%|█████▏    | 215107/414113 [00:47<00:42, 4656.49it/s]\u001b[A\n",
      " 52%|█████▏    | 215575/414113 [00:48<00:42, 4662.99it/s]\u001b[A\n",
      " 52%|█████▏    | 216042/414113 [00:48<00:42, 4646.44it/s]\u001b[A\n",
      " 52%|█████▏    | 216510/414113 [00:48<00:42, 4654.73it/s]\u001b[A\n",
      " 52%|█████▏    | 216976/414113 [00:48<00:42, 4651.25it/s]\u001b[A\n",
      " 53%|█████▎    | 217445/414113 [00:48<00:42, 4661.12it/s]\u001b[A\n",
      " 53%|█████▎    | 217912/414113 [00:48<00:42, 4661.51it/s]\u001b[A\n",
      " 53%|█████▎    | 218382/414113 [00:48<00:41, 4671.04it/s]\u001b[A\n",
      " 53%|█████▎    | 218853/414113 [00:48<00:41, 4682.24it/s]\u001b[A\n",
      " 53%|█████▎    | 219326/414113 [00:48<00:41, 4696.11it/s]\u001b[A\n",
      " 53%|█████▎    | 219796/414113 [00:48<00:41, 4665.80it/s]\u001b[A\n",
      " 53%|█████▎    | 220263/414113 [00:49<00:41, 4656.58it/s]\u001b[A\n",
      " 53%|█████▎    | 220729/414113 [00:49<00:41, 4620.01it/s]\u001b[A\n",
      " 53%|█████▎    | 221192/414113 [00:49<00:42, 4577.10it/s]\u001b[A\n",
      " 54%|█████▎    | 221655/414113 [00:49<00:41, 4590.74it/s]\u001b[A\n",
      " 54%|█████▎    | 222122/414113 [00:49<00:41, 4612.49it/s]\u001b[A\n",
      " 54%|█████▍    | 222588/414113 [00:49<00:41, 4624.35it/s]\u001b[A\n",
      " 54%|█████▍    | 223051/414113 [00:49<00:41, 4625.42it/s]\u001b[A\n",
      " 54%|█████▍    | 223518/414113 [00:49<00:41, 4638.02it/s]\u001b[A\n",
      " 54%|█████▍    | 223982/414113 [00:49<00:42, 4514.24it/s]\u001b[A\n",
      " 54%|█████▍    | 224452/414113 [00:50<00:41, 4566.85it/s]\u001b[A\n",
      " 54%|█████▍    | 224915/414113 [00:50<00:41, 4584.21it/s]\u001b[A\n",
      " 54%|█████▍    | 225384/414113 [00:50<00:40, 4614.95it/s]\u001b[A\n",
      " 55%|█████▍    | 225846/414113 [00:50<00:41, 4580.72it/s]\u001b[A\n",
      " 55%|█████▍    | 226315/414113 [00:50<00:40, 4612.34it/s]\u001b[A\n",
      " 55%|█████▍    | 226782/414113 [00:50<00:40, 4628.34it/s]\u001b[A\n",
      " 55%|█████▍    | 227249/414113 [00:50<00:40, 4639.60it/s]\u001b[A\n",
      " 55%|█████▍    | 227719/414113 [00:50<00:40, 4657.52it/s]\u001b[A\n",
      " 55%|█████▌    | 228185/414113 [00:50<00:40, 4620.78it/s]\u001b[A\n",
      " 55%|█████▌    | 228655/414113 [00:50<00:39, 4641.84it/s]\u001b[A\n",
      " 55%|█████▌    | 229120/414113 [00:51<00:40, 4622.87it/s]\u001b[A\n",
      " 55%|█████▌    | 229583/414113 [00:51<00:39, 4620.41it/s]\u001b[A\n",
      " 56%|█████▌    | 230046/414113 [00:51<00:40, 4581.88it/s]\u001b[A\n",
      " 56%|█████▌    | 230508/414113 [00:51<00:39, 4591.53it/s]\u001b[A\n",
      " 56%|█████▌    | 230968/414113 [00:51<00:40, 4567.84it/s]\u001b[A\n",
      " 56%|█████▌    | 231425/414113 [00:51<00:40, 4522.72it/s]\u001b[A\n",
      " 56%|█████▌    | 231880/414113 [00:51<00:40, 4530.69it/s]\u001b[A\n",
      " 56%|█████▌    | 232334/414113 [00:51<00:40, 4523.95it/s]\u001b[A\n",
      " 56%|█████▌    | 232791/414113 [00:51<00:39, 4537.44it/s]\u001b[A\n",
      " 56%|█████▋    | 233253/414113 [00:51<00:39, 4560.89it/s]\u001b[A\n",
      " 56%|█████▋    | 233710/414113 [00:52<00:39, 4558.62it/s]\u001b[A\n",
      " 57%|█████▋    | 234178/414113 [00:52<00:39, 4592.12it/s]\u001b[A\n",
      " 57%|█████▋    | 234638/414113 [00:52<00:39, 4580.95it/s]\u001b[A\n",
      " 57%|█████▋    | 235101/414113 [00:52<00:38, 4593.40it/s]\u001b[A\n",
      " 57%|█████▋    | 235561/414113 [00:52<00:38, 4585.71it/s]\u001b[A\n",
      " 57%|█████▋    | 236020/414113 [00:52<00:38, 4571.37it/s]\u001b[A\n",
      " 57%|█████▋    | 236478/414113 [00:52<00:39, 4548.47it/s]\u001b[A\n",
      " 57%|█████▋    | 236933/414113 [00:52<00:39, 4512.50it/s]\u001b[A\n",
      " 57%|█████▋    | 237393/414113 [00:52<00:38, 4535.16it/s]\u001b[A\n",
      " 57%|█████▋    | 237847/414113 [00:52<00:39, 4509.53it/s]\u001b[A\n",
      " 58%|█████▊    | 238309/414113 [00:53<00:38, 4540.53it/s]\u001b[A\n",
      " 58%|█████▊    | 238764/414113 [00:53<00:38, 4517.82it/s]\u001b[A\n",
      " 58%|█████▊    | 239231/414113 [00:53<00:38, 4560.49it/s]\u001b[A\n",
      " 58%|█████▊    | 239688/414113 [00:53<00:38, 4537.94it/s]\u001b[A\n",
      " 58%|█████▊    | 240149/414113 [00:53<00:38, 4557.10it/s]\u001b[A\n",
      " 58%|█████▊    | 240605/414113 [00:53<00:38, 4542.98it/s]\u001b[A\n",
      " 58%|█████▊    | 241069/414113 [00:53<00:37, 4571.64it/s]\u001b[A\n",
      " 58%|█████▊    | 241527/414113 [00:53<00:38, 4524.94it/s]\u001b[A\n",
      " 58%|█████▊    | 241989/414113 [00:53<00:37, 4551.84it/s]\u001b[A\n",
      " 59%|█████▊    | 242450/414113 [00:53<00:37, 4566.81it/s]\u001b[A\n",
      " 59%|█████▊    | 242910/414113 [00:54<00:37, 4575.32it/s]\u001b[A\n",
      " 59%|█████▉    | 243368/414113 [00:54<00:37, 4553.22it/s]\u001b[A\n",
      " 59%|█████▉    | 243824/414113 [00:54<00:37, 4539.97it/s]\u001b[A\n",
      " 59%|█████▉    | 244279/414113 [00:54<00:37, 4536.91it/s]\u001b[A\n",
      " 59%|█████▉    | 244734/414113 [00:54<00:37, 4540.04it/s]\u001b[A\n",
      " 59%|█████▉    | 245189/414113 [00:54<00:37, 4526.22it/s]\u001b[A\n",
      " 59%|█████▉    | 245642/414113 [00:54<00:38, 4394.14it/s]\u001b[A\n",
      " 59%|█████▉    | 246095/414113 [00:54<00:37, 4432.04it/s]\u001b[A\n",
      " 60%|█████▉    | 246547/414113 [00:54<00:37, 4456.19it/s]\u001b[A\n",
      " 60%|█████▉    | 246994/414113 [00:54<00:37, 4447.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 247458/414113 [00:55<00:37, 4502.99it/s]\u001b[A\n",
      " 60%|█████▉    | 247922/414113 [00:55<00:36, 4542.41it/s]\u001b[A\n",
      " 60%|█████▉    | 248377/414113 [00:55<00:36, 4531.33it/s]\u001b[A\n",
      " 60%|██████    | 248844/414113 [00:55<00:36, 4569.87it/s]\u001b[A\n",
      " 60%|██████    | 249306/414113 [00:55<00:35, 4583.16it/s]\u001b[A\n",
      " 60%|██████    | 249765/414113 [00:55<00:35, 4567.31it/s]\u001b[A\n",
      " 60%|██████    | 250222/414113 [00:55<00:36, 4550.27it/s]\u001b[A\n",
      " 61%|██████    | 250678/414113 [00:55<00:36, 4527.35it/s]\u001b[A\n",
      " 61%|██████    | 251131/414113 [00:55<00:36, 4527.22it/s]\u001b[A\n",
      " 61%|██████    | 251588/414113 [00:55<00:35, 4538.40it/s]\u001b[A\n",
      " 61%|██████    | 252042/414113 [00:56<00:36, 4492.23it/s]\u001b[A\n",
      " 61%|██████    | 252496/414113 [00:56<00:35, 4506.41it/s]\u001b[A\n",
      " 61%|██████    | 252948/414113 [00:56<00:35, 4508.05it/s]\u001b[A\n",
      " 61%|██████    | 253399/414113 [00:56<00:36, 4386.55it/s]\u001b[A\n",
      " 61%|██████▏   | 253858/414113 [00:56<00:36, 4443.11it/s]\u001b[A\n",
      " 61%|██████▏   | 254311/414113 [00:56<00:35, 4467.31it/s]\u001b[A\n",
      " 62%|██████▏   | 254775/414113 [00:56<00:35, 4516.32it/s]\u001b[A\n",
      " 62%|██████▏   | 255244/414113 [00:56<00:34, 4565.35it/s]\u001b[A\n",
      " 62%|██████▏   | 255709/414113 [00:56<00:34, 4587.48it/s]\u001b[A\n",
      " 62%|██████▏   | 256171/414113 [00:56<00:34, 4596.71it/s]\u001b[A\n",
      " 62%|██████▏   | 256641/414113 [00:57<00:34, 4626.71it/s]\u001b[A\n",
      " 62%|██████▏   | 257114/414113 [00:57<00:33, 4654.34it/s]\u001b[A\n",
      " 62%|██████▏   | 257580/414113 [00:57<00:33, 4632.97it/s]\u001b[A\n",
      " 62%|██████▏   | 258044/414113 [00:57<00:33, 4619.82it/s]\u001b[A\n",
      " 62%|██████▏   | 258507/414113 [00:57<00:33, 4598.78it/s]\u001b[A\n",
      " 63%|██████▎   | 258967/414113 [00:57<00:34, 4553.90it/s]\u001b[A\n",
      " 63%|██████▎   | 259433/414113 [00:57<00:33, 4585.03it/s]\u001b[A\n",
      " 63%|██████▎   | 259894/414113 [00:57<00:33, 4590.72it/s]\u001b[A\n",
      " 63%|██████▎   | 260354/414113 [00:57<00:34, 4453.85it/s]\u001b[A\n",
      " 63%|██████▎   | 260826/414113 [00:57<00:33, 4528.66it/s]\u001b[A\n",
      " 63%|██████▎   | 261299/414113 [00:58<00:33, 4585.97it/s]\u001b[A\n",
      " 63%|██████▎   | 261759/414113 [00:58<00:33, 4581.33it/s]\u001b[A\n",
      " 63%|██████▎   | 262220/414113 [00:58<00:33, 4589.20it/s]\u001b[A\n",
      " 63%|██████▎   | 262680/414113 [00:58<00:32, 4590.02it/s]\u001b[A\n",
      " 64%|██████▎   | 263140/414113 [00:58<00:33, 4560.61it/s]\u001b[A\n",
      " 64%|██████▎   | 263597/414113 [00:58<00:33, 4519.39it/s]\u001b[A\n",
      " 64%|██████▍   | 264054/414113 [00:58<00:33, 4531.74it/s]\u001b[A\n",
      " 64%|██████▍   | 264508/414113 [00:58<00:33, 4509.71it/s]\u001b[A\n",
      " 64%|██████▍   | 264960/414113 [00:58<00:33, 4479.27it/s]\u001b[A\n",
      " 64%|██████▍   | 265411/414113 [00:59<00:33, 4487.92it/s]\u001b[A\n",
      " 64%|██████▍   | 265861/414113 [00:59<00:33, 4490.24it/s]\u001b[A\n",
      " 64%|██████▍   | 266313/414113 [00:59<00:32, 4498.01it/s]\u001b[A\n",
      " 64%|██████▍   | 266771/414113 [00:59<00:32, 4522.06it/s]\u001b[A\n",
      " 65%|██████▍   | 267225/414113 [00:59<00:32, 4526.05it/s]\u001b[A\n",
      " 65%|██████▍   | 267682/414113 [00:59<00:32, 4536.40it/s]\u001b[A\n",
      " 65%|██████▍   | 268136/414113 [00:59<00:32, 4523.30it/s]\u001b[A\n",
      " 65%|██████▍   | 268599/414113 [00:59<00:31, 4552.53it/s]\u001b[A\n",
      " 65%|██████▍   | 269060/414113 [00:59<00:31, 4567.41it/s]\u001b[A\n",
      " 65%|██████▌   | 269517/414113 [00:59<00:31, 4563.41it/s]\u001b[A\n",
      " 65%|██████▌   | 269974/414113 [01:00<00:31, 4552.30it/s]\u001b[A\n",
      " 65%|██████▌   | 270435/414113 [01:00<00:31, 4568.06it/s]\u001b[A\n",
      " 65%|██████▌   | 270898/414113 [01:00<00:31, 4585.43it/s]\u001b[A\n",
      " 66%|██████▌   | 271357/414113 [01:00<00:31, 4549.50it/s]\u001b[A\n",
      " 66%|██████▌   | 271820/414113 [01:00<00:31, 4572.21it/s]\u001b[A\n",
      " 66%|██████▌   | 272278/414113 [01:00<00:31, 4539.70it/s]\u001b[A\n",
      " 66%|██████▌   | 272733/414113 [01:00<00:31, 4537.68it/s]\u001b[A\n",
      " 66%|██████▌   | 273188/414113 [01:00<00:31, 4538.71it/s]\u001b[A\n",
      " 66%|██████▌   | 273642/414113 [01:00<00:30, 4531.44it/s]\u001b[A\n",
      " 66%|██████▌   | 274096/414113 [01:00<00:31, 4501.81it/s]\u001b[A\n",
      " 66%|██████▋   | 274557/414113 [01:01<00:30, 4532.10it/s]\u001b[A\n",
      " 66%|██████▋   | 275018/414113 [01:01<00:30, 4554.32it/s]\u001b[A\n",
      " 67%|██████▋   | 275474/414113 [01:01<00:30, 4533.95it/s]\u001b[A\n",
      " 67%|██████▋   | 275938/414113 [01:01<00:30, 4563.65it/s]\u001b[A\n",
      " 67%|██████▋   | 276402/414113 [01:01<00:30, 4586.27it/s]\u001b[A\n",
      " 67%|██████▋   | 276861/414113 [01:01<00:30, 4560.80it/s]\u001b[A\n",
      " 67%|██████▋   | 277321/414113 [01:01<00:29, 4572.22it/s]\u001b[A\n",
      " 67%|██████▋   | 277779/414113 [01:01<00:29, 4561.58it/s]\u001b[A\n",
      " 67%|██████▋   | 278236/414113 [01:01<00:29, 4553.45it/s]\u001b[A\n",
      " 67%|██████▋   | 278692/414113 [01:01<00:29, 4538.14it/s]\u001b[A\n",
      " 67%|██████▋   | 279151/414113 [01:02<00:29, 4551.38it/s]\u001b[A\n",
      " 68%|██████▊   | 279609/414113 [01:02<00:29, 4558.58it/s]\u001b[A\n",
      " 68%|██████▊   | 280068/414113 [01:02<00:29, 4566.08it/s]\u001b[A\n",
      " 68%|██████▊   | 280532/414113 [01:02<00:29, 4587.11it/s]\u001b[A\n",
      " 68%|██████▊   | 280991/414113 [01:02<00:29, 4572.37it/s]\u001b[A\n",
      " 68%|██████▊   | 281449/414113 [01:02<00:29, 4542.32it/s]\u001b[A\n",
      " 68%|██████▊   | 281907/414113 [01:02<00:29, 4553.18it/s]\u001b[A\n",
      " 68%|██████▊   | 282363/414113 [01:02<00:28, 4549.64it/s]\u001b[A\n",
      " 68%|██████▊   | 282819/414113 [01:02<00:28, 4552.48it/s]\u001b[A\n",
      " 68%|██████▊   | 283280/414113 [01:02<00:28, 4568.49it/s]\u001b[A\n",
      " 69%|██████▊   | 283741/414113 [01:03<00:28, 4580.22it/s]\u001b[A\n",
      " 69%|██████▊   | 284202/414113 [01:03<00:28, 4587.82it/s]\u001b[A\n",
      " 69%|██████▊   | 284662/414113 [01:03<00:28, 4589.32it/s]\u001b[A\n",
      " 69%|██████▉   | 285127/414113 [01:03<00:28, 4606.42it/s]\u001b[A\n",
      " 69%|██████▉   | 285592/414113 [01:03<00:27, 4618.26it/s]\u001b[A\n",
      " 69%|██████▉   | 286061/414113 [01:03<00:27, 4638.19it/s]\u001b[A\n",
      " 69%|██████▉   | 286525/414113 [01:03<00:27, 4609.46it/s]\u001b[A\n",
      " 69%|██████▉   | 286987/414113 [01:03<00:27, 4591.78it/s]\u001b[A\n",
      " 69%|██████▉   | 287456/414113 [01:03<00:27, 4619.13it/s]\u001b[A\n",
      " 70%|██████▉   | 287928/414113 [01:03<00:27, 4646.34it/s]\u001b[A\n",
      " 70%|██████▉   | 288395/414113 [01:04<00:27, 4650.93it/s]\u001b[A\n",
      " 70%|██████▉   | 288861/414113 [01:04<00:27, 4627.49it/s]\u001b[A\n",
      " 70%|██████▉   | 289333/414113 [01:04<00:26, 4653.91it/s]\u001b[A\n",
      " 70%|██████▉   | 289800/414113 [01:04<00:26, 4656.26it/s]\u001b[A\n",
      " 70%|███████   | 290266/414113 [01:04<00:26, 4643.72it/s]\u001b[A\n",
      " 70%|███████   | 290732/414113 [01:04<00:26, 4646.62it/s]\u001b[A\n",
      " 70%|███████   | 291197/414113 [01:04<00:26, 4608.95it/s]\u001b[A\n",
      " 70%|███████   | 291658/414113 [01:04<00:26, 4576.24it/s]\u001b[A\n",
      " 71%|███████   | 292129/414113 [01:04<00:26, 4613.40it/s]\u001b[A\n",
      " 71%|███████   | 292591/414113 [01:04<00:26, 4597.29it/s]\u001b[A\n",
      " 71%|███████   | 293063/414113 [01:05<00:26, 4632.76it/s]\u001b[A\n",
      " 71%|███████   | 293530/414113 [01:05<00:25, 4642.97it/s]\u001b[A\n",
      " 71%|███████   | 293995/414113 [01:05<00:25, 4632.56it/s]\u001b[A\n",
      " 71%|███████   | 294459/414113 [01:05<00:25, 4630.52it/s]\u001b[A\n",
      " 71%|███████   | 294923/414113 [01:05<00:26, 4542.50it/s]\u001b[A\n",
      " 71%|███████▏  | 295378/414113 [01:05<00:27, 4336.11it/s]\u001b[A\n",
      " 71%|███████▏  | 295843/414113 [01:05<00:26, 4424.59it/s]\u001b[A\n",
      " 72%|███████▏  | 296290/414113 [01:05<00:26, 4436.11it/s]\u001b[A\n",
      " 72%|███████▏  | 296758/414113 [01:05<00:26, 4504.95it/s]\u001b[A\n",
      " 72%|███████▏  | 297210/414113 [01:05<00:25, 4507.04it/s]\u001b[A\n",
      " 72%|███████▏  | 297662/414113 [01:06<00:25, 4479.57it/s]\u001b[A\n",
      " 72%|███████▏  | 298111/414113 [01:06<00:25, 4466.05it/s]\u001b[A\n",
      " 72%|███████▏  | 298572/414113 [01:06<00:25, 4506.13it/s]\u001b[A\n",
      " 72%|███████▏  | 299031/414113 [01:06<00:25, 4530.89it/s]\u001b[A\n",
      " 72%|███████▏  | 299492/414113 [01:06<00:25, 4551.62it/s]\u001b[A\n",
      " 72%|███████▏  | 299958/414113 [01:06<00:24, 4581.22it/s]\u001b[A\n",
      " 73%|███████▎  | 300417/414113 [01:06<00:25, 4530.50it/s]\u001b[A\n",
      " 73%|███████▎  | 300873/414113 [01:06<00:24, 4536.76it/s]\u001b[A\n",
      " 73%|███████▎  | 301333/414113 [01:06<00:24, 4554.98it/s]\u001b[A\n",
      " 73%|███████▎  | 301790/414113 [01:06<00:24, 4558.19it/s]\u001b[A\n",
      " 73%|███████▎  | 302246/414113 [01:07<00:24, 4543.28it/s]\u001b[A\n",
      " 73%|███████▎  | 302705/414113 [01:07<00:24, 4556.00it/s]\u001b[A\n",
      " 73%|███████▎  | 303164/414113 [01:07<00:24, 4564.79it/s]\u001b[A\n",
      " 73%|███████▎  | 303630/414113 [01:07<00:46, 2368.08it/s]\u001b[A\n",
      " 73%|███████▎  | 304083/414113 [01:07<00:39, 2763.28it/s]\u001b[A\n",
      " 74%|███████▎  | 304534/414113 [01:07<00:35, 3126.07it/s]\u001b[A\n",
      " 74%|███████▎  | 304994/414113 [01:07<00:31, 3457.97it/s]\u001b[A\n",
      " 74%|███████▍  | 305444/414113 [01:08<00:29, 3715.75it/s]\u001b[A\n",
      " 74%|███████▍  | 305894/414113 [01:08<00:27, 3918.87it/s]\u001b[A\n",
      " 74%|███████▍  | 306345/414113 [01:08<00:26, 4078.13it/s]\u001b[A\n",
      " 74%|███████▍  | 306796/414113 [01:08<00:25, 4198.16it/s]\u001b[A\n",
      " 74%|███████▍  | 307259/414113 [01:08<00:24, 4317.53it/s]\u001b[A\n",
      " 74%|███████▍  | 307708/414113 [01:08<00:24, 4333.47it/s]\u001b[A\n",
      " 74%|███████▍  | 308168/414113 [01:08<00:24, 4409.82it/s]\u001b[A\n",
      " 75%|███████▍  | 308631/414113 [01:08<00:23, 4471.98it/s]\u001b[A\n",
      " 75%|███████▍  | 309090/414113 [01:08<00:23, 4506.37it/s]\u001b[A\n",
      " 75%|███████▍  | 309555/414113 [01:08<00:22, 4547.26it/s]\u001b[A\n",
      " 75%|███████▍  | 310021/414113 [01:09<00:22, 4578.34it/s]\u001b[A\n",
      " 75%|███████▍  | 310484/414113 [01:09<00:22, 4593.68it/s]\u001b[A\n",
      " 75%|███████▌  | 310945/414113 [01:09<00:22, 4577.14it/s]\u001b[A\n",
      " 75%|███████▌  | 311404/414113 [01:09<00:22, 4575.83it/s]\u001b[A\n",
      " 75%|███████▌  | 311863/414113 [01:09<00:22, 4575.76it/s]\u001b[A\n",
      " 75%|███████▌  | 312323/414113 [01:09<00:22, 4580.32it/s]\u001b[A\n",
      " 76%|███████▌  | 312782/414113 [01:09<00:22, 4555.94it/s]\u001b[A\n",
      " 76%|███████▌  | 313238/414113 [01:09<00:22, 4549.00it/s]\u001b[A\n",
      " 76%|███████▌  | 313694/414113 [01:09<00:22, 4533.72it/s]\u001b[A\n",
      " 76%|███████▌  | 314153/414113 [01:10<00:21, 4549.74it/s]\u001b[A\n",
      " 76%|███████▌  | 314612/414113 [01:10<00:21, 4558.84it/s]\u001b[A\n",
      " 76%|███████▌  | 315071/414113 [01:10<00:21, 4566.87it/s]\u001b[A\n",
      " 76%|███████▌  | 315534/414113 [01:10<00:21, 4584.58it/s]\u001b[A\n",
      " 76%|███████▋  | 315996/414113 [01:10<00:21, 4594.52it/s]\u001b[A\n",
      " 76%|███████▋  | 316463/414113 [01:10<00:21, 4616.54it/s]\u001b[A\n",
      " 77%|███████▋  | 316928/414113 [01:10<00:21, 4623.89it/s]\u001b[A\n",
      " 77%|███████▋  | 317391/414113 [01:10<00:21, 4594.58it/s]\u001b[A\n",
      " 77%|███████▋  | 317851/414113 [01:10<00:20, 4594.16it/s]\u001b[A\n",
      " 77%|███████▋  | 318312/414113 [01:10<00:20, 4597.65it/s]\u001b[A\n",
      " 77%|███████▋  | 318777/414113 [01:11<00:20, 4610.95it/s]\u001b[A\n",
      " 77%|███████▋  | 319244/414113 [01:11<00:20, 4626.31it/s]\u001b[A\n",
      " 77%|███████▋  | 319714/414113 [01:11<00:20, 4647.50it/s]\u001b[A\n",
      " 77%|███████▋  | 320179/414113 [01:11<00:20, 4642.69it/s]\u001b[A\n",
      " 77%|███████▋  | 320647/414113 [01:11<00:20, 4653.00it/s]\u001b[A\n",
      " 78%|███████▊  | 321113/414113 [01:11<00:20, 4610.12it/s]\u001b[A\n",
      " 78%|███████▊  | 321575/414113 [01:11<00:20, 4593.42it/s]\u001b[A\n",
      " 78%|███████▊  | 322035/414113 [01:11<00:21, 4354.96it/s]\u001b[A\n",
      " 78%|███████▊  | 322505/414113 [01:11<00:20, 4451.65it/s]\u001b[A\n",
      " 78%|███████▊  | 322970/414113 [01:11<00:20, 4507.99it/s]\u001b[A\n",
      " 78%|███████▊  | 323437/414113 [01:12<00:19, 4553.47it/s]\u001b[A\n",
      " 78%|███████▊  | 323896/414113 [01:12<00:19, 4563.18it/s]\u001b[A\n",
      " 78%|███████▊  | 324361/414113 [01:12<00:19, 4586.10it/s]\u001b[A\n",
      " 78%|███████▊  | 324833/414113 [01:12<00:19, 4623.38it/s]\u001b[A\n",
      " 79%|███████▊  | 325296/414113 [01:12<00:19, 4620.30it/s]\u001b[A\n",
      " 79%|███████▊  | 325759/414113 [01:12<00:19, 4612.29it/s]\u001b[A\n",
      " 79%|███████▉  | 326221/414113 [01:12<00:19, 4603.87it/s]\u001b[A\n",
      " 79%|███████▉  | 326682/414113 [01:12<00:19, 4575.69it/s]\u001b[A\n",
      " 79%|███████▉  | 327142/414113 [01:12<00:18, 4580.32it/s]\u001b[A\n",
      " 79%|███████▉  | 327601/414113 [01:12<00:19, 4548.45it/s]\u001b[A\n",
      " 79%|███████▉  | 328056/414113 [01:13<00:19, 4303.26it/s]\u001b[A\n",
      " 79%|███████▉  | 328515/414113 [01:13<00:19, 4384.06it/s]\u001b[A\n",
      " 79%|███████▉  | 328956/414113 [01:13<00:19, 4362.15it/s]\u001b[A\n",
      " 80%|███████▉  | 329394/414113 [01:13<00:19, 4367.05it/s]\u001b[A\n",
      " 80%|███████▉  | 329843/414113 [01:13<00:19, 4400.67it/s]\u001b[A\n",
      " 80%|███████▉  | 330284/414113 [01:13<00:19, 4393.82it/s]\u001b[A\n",
      " 80%|███████▉  | 330730/414113 [01:13<00:18, 4413.10it/s]\u001b[A\n",
      " 80%|███████▉  | 331172/414113 [01:13<00:18, 4391.48it/s]\u001b[A\n",
      " 80%|████████  | 331613/414113 [01:13<00:18, 4396.57it/s]\u001b[A\n",
      " 80%|████████  | 332061/414113 [01:13<00:18, 4420.61it/s]\u001b[A\n",
      " 80%|████████  | 332504/414113 [01:14<00:18, 4380.90it/s]\u001b[A\n",
      " 80%|████████  | 332946/414113 [01:14<00:18, 4390.54it/s]\u001b[A\n",
      " 81%|████████  | 333390/414113 [01:14<00:18, 4404.38it/s]\u001b[A\n",
      " 81%|████████  | 333837/414113 [01:14<00:18, 4421.92it/s]\u001b[A\n",
      " 81%|████████  | 334299/414113 [01:14<00:17, 4478.37it/s]\u001b[A\n",
      " 81%|████████  | 334751/414113 [01:14<00:17, 4488.22it/s]\u001b[A\n",
      " 81%|████████  | 335200/414113 [01:14<00:17, 4454.24it/s]\u001b[A\n",
      " 81%|████████  | 335646/414113 [01:14<00:17, 4409.52it/s]\u001b[A\n",
      " 81%|████████  | 336093/414113 [01:14<00:17, 4425.70it/s]\u001b[A\n",
      " 81%|████████▏ | 336538/414113 [01:14<00:17, 4430.62it/s]\u001b[A\n",
      " 81%|████████▏ | 336982/414113 [01:15<00:17, 4421.16it/s]\u001b[A\n",
      " 81%|████████▏ | 337425/414113 [01:15<00:17, 4413.89it/s]\u001b[A\n",
      " 82%|████████▏ | 337867/414113 [01:15<00:17, 4412.36it/s]\u001b[A\n",
      " 82%|████████▏ | 338309/414113 [01:15<00:17, 4403.31it/s]\u001b[A\n",
      " 82%|████████▏ | 338750/414113 [01:15<00:17, 4362.87it/s]\u001b[A\n",
      " 82%|████████▏ | 339192/414113 [01:15<00:17, 4378.76it/s]\u001b[A\n",
      " 82%|████████▏ | 339630/414113 [01:15<00:17, 4357.08it/s]\u001b[A\n",
      " 82%|████████▏ | 340077/414113 [01:15<00:16, 4390.18it/s]\u001b[A\n",
      " 82%|████████▏ | 340517/414113 [01:15<00:16, 4378.36it/s]\u001b[A\n",
      " 82%|████████▏ | 340955/414113 [01:15<00:16, 4375.64it/s]\u001b[A\n",
      " 82%|████████▏ | 341417/414113 [01:16<00:16, 4443.97it/s]\u001b[A\n",
      " 83%|████████▎ | 341862/414113 [01:16<00:16, 4284.56it/s]\u001b[A\n",
      " 83%|████████▎ | 342304/414113 [01:16<00:16, 4322.21it/s]\u001b[A\n",
      " 83%|████████▎ | 342738/414113 [01:16<00:16, 4327.12it/s]\u001b[A\n",
      " 83%|████████▎ | 343191/414113 [01:16<00:16, 4384.02it/s]\u001b[A\n",
      " 83%|████████▎ | 343635/414113 [01:16<00:16, 4399.48it/s]\u001b[A\n",
      " 83%|████████▎ | 344090/414113 [01:16<00:15, 4441.84it/s]\u001b[A\n",
      " 83%|████████▎ | 344545/414113 [01:16<00:15, 4472.75it/s]\u001b[A\n",
      " 83%|████████▎ | 345008/414113 [01:16<00:15, 4517.08it/s]\u001b[A\n",
      " 83%|████████▎ | 345461/414113 [01:17<00:15, 4496.24it/s]\u001b[A\n",
      " 84%|████████▎ | 345916/414113 [01:17<00:15, 4510.07it/s]\u001b[A\n",
      " 84%|████████▎ | 346383/414113 [01:17<00:14, 4556.71it/s]\u001b[A\n",
      " 84%|████████▍ | 346839/414113 [01:17<00:14, 4539.97it/s]\u001b[A\n",
      " 84%|████████▍ | 347301/414113 [01:17<00:14, 4563.11it/s]\u001b[A\n",
      " 84%|████████▍ | 347758/414113 [01:17<00:14, 4497.21it/s]\u001b[A\n",
      " 84%|████████▍ | 348209/414113 [01:17<00:14, 4458.38it/s]\u001b[A\n",
      " 84%|████████▍ | 348656/414113 [01:17<00:14, 4439.42it/s]\u001b[A\n",
      " 84%|████████▍ | 349115/414113 [01:17<00:14, 4482.97it/s]\u001b[A\n",
      " 84%|████████▍ | 349564/414113 [01:17<00:14, 4475.63it/s]\u001b[A\n",
      " 85%|████████▍ | 350022/414113 [01:18<00:14, 4504.74it/s]\u001b[A\n",
      " 85%|████████▍ | 350479/414113 [01:18<00:14, 4523.68it/s]\u001b[A\n",
      " 85%|████████▍ | 350942/414113 [01:18<00:13, 4553.73it/s]\u001b[A\n",
      " 85%|████████▍ | 351403/414113 [01:18<00:13, 4568.21it/s]\u001b[A\n",
      " 85%|████████▍ | 351860/414113 [01:18<00:13, 4567.04it/s]\u001b[A\n",
      " 85%|████████▌ | 352317/414113 [01:18<00:13, 4541.09it/s]\u001b[A\n",
      " 85%|████████▌ | 352772/414113 [01:18<00:13, 4514.23it/s]\u001b[A\n",
      " 85%|████████▌ | 353227/414113 [01:18<00:13, 4524.31it/s]\u001b[A\n",
      " 85%|████████▌ | 353680/414113 [01:18<00:13, 4485.39it/s]\u001b[A\n",
      " 86%|████████▌ | 354134/414113 [01:18<00:13, 4499.56it/s]\u001b[A\n",
      " 86%|████████▌ | 354603/414113 [01:19<00:13, 4554.31it/s]\u001b[A\n",
      " 86%|████████▌ | 355066/414113 [01:19<00:12, 4574.73it/s]\u001b[A\n",
      " 86%|████████▌ | 355524/414113 [01:19<00:12, 4556.35it/s]\u001b[A\n",
      " 86%|████████▌ | 355980/414113 [01:19<00:12, 4544.97it/s]\u001b[A\n",
      " 86%|████████▌ | 356435/414113 [01:19<00:12, 4534.89it/s]\u001b[A\n",
      " 86%|████████▌ | 356889/414113 [01:19<00:12, 4528.03it/s]\u001b[A\n",
      " 86%|████████▋ | 357342/414113 [01:19<00:12, 4462.67it/s]\u001b[A\n",
      " 86%|████████▋ | 357805/414113 [01:19<00:12, 4511.57it/s]\u001b[A\n",
      " 87%|████████▋ | 358257/414113 [01:19<00:12, 4494.80it/s]\u001b[A\n",
      " 87%|████████▋ | 358718/414113 [01:19<00:12, 4526.70it/s]\u001b[A\n",
      " 87%|████████▋ | 359180/414113 [01:20<00:12, 4553.64it/s]\u001b[A\n",
      " 87%|████████▋ | 359644/414113 [01:20<00:11, 4576.83it/s]\u001b[A\n",
      " 87%|████████▋ | 360102/414113 [01:20<00:11, 4549.66it/s]\u001b[A\n",
      " 87%|████████▋ | 360561/414113 [01:20<00:11, 4559.30it/s]\u001b[A\n",
      " 87%|████████▋ | 361018/414113 [01:20<00:11, 4546.31it/s]\u001b[A\n",
      " 87%|████████▋ | 361473/414113 [01:20<00:11, 4544.40it/s]\u001b[A\n",
      " 87%|████████▋ | 361928/414113 [01:20<00:11, 4516.77it/s]\u001b[A\n",
      " 88%|████████▊ | 362383/414113 [01:20<00:11, 4525.34it/s]\u001b[A\n",
      " 88%|████████▊ | 362837/414113 [01:20<00:11, 4529.37it/s]\u001b[A\n",
      " 88%|████████▊ | 363290/414113 [01:20<00:11, 4521.66it/s]\u001b[A\n",
      " 88%|████████▊ | 363743/414113 [01:21<00:11, 4508.83it/s]\u001b[A\n",
      " 88%|████████▊ | 364197/414113 [01:21<00:11, 4517.04it/s]\u001b[A\n",
      " 88%|████████▊ | 364649/414113 [01:21<00:10, 4502.83it/s]\u001b[A\n",
      " 88%|████████▊ | 365105/414113 [01:21<00:10, 4517.04it/s]\u001b[A\n",
      " 88%|████████▊ | 365557/414113 [01:21<00:10, 4509.19it/s]\u001b[A\n",
      " 88%|████████▊ | 366008/414113 [01:21<00:10, 4509.09it/s]\u001b[A\n",
      " 88%|████████▊ | 366460/414113 [01:21<00:10, 4511.60it/s]\u001b[A\n",
      " 89%|████████▊ | 366912/414113 [01:21<00:10, 4506.84it/s]\u001b[A\n",
      " 89%|████████▊ | 367366/414113 [01:21<00:10, 4515.31it/s]\u001b[A\n",
      " 89%|████████▉ | 367828/414113 [01:21<00:10, 4545.96it/s]\u001b[A\n",
      " 89%|████████▉ | 368283/414113 [01:22<00:10, 4543.64it/s]\u001b[A\n",
      " 89%|████████▉ | 368747/414113 [01:22<00:09, 4572.01it/s]\u001b[A\n",
      " 89%|████████▉ | 369205/414113 [01:22<00:09, 4570.07it/s]\u001b[A\n",
      " 89%|████████▉ | 369663/414113 [01:22<00:09, 4548.42it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 370118/414113 [01:22<00:09, 4534.65it/s]\u001b[A\n",
      " 89%|████████▉ | 370572/414113 [01:22<00:09, 4485.18it/s]\u001b[A\n",
      " 90%|████████▉ | 371021/414113 [01:22<00:09, 4485.71it/s]\u001b[A\n",
      " 90%|████████▉ | 371470/414113 [01:22<00:09, 4465.38it/s]\u001b[A\n",
      " 90%|████████▉ | 371920/414113 [01:22<00:09, 4475.09it/s]\u001b[A\n",
      " 90%|████████▉ | 372368/414113 [01:22<00:09, 4444.47it/s]\u001b[A\n",
      " 90%|█████████ | 372819/414113 [01:23<00:09, 4462.60it/s]\u001b[A\n",
      " 90%|█████████ | 373266/414113 [01:23<00:09, 4288.04it/s]\u001b[A\n",
      " 90%|█████████ | 373700/414113 [01:23<00:09, 4300.80it/s]\u001b[A\n",
      " 90%|█████████ | 374137/414113 [01:23<00:09, 4320.36it/s]\u001b[A\n",
      " 90%|█████████ | 374587/414113 [01:23<00:09, 4371.27it/s]\u001b[A\n",
      " 91%|█████████ | 375041/414113 [01:23<00:08, 4419.06it/s]\u001b[A\n",
      " 91%|█████████ | 375484/414113 [01:23<00:08, 4419.42it/s]\u001b[A\n",
      " 91%|█████████ | 375944/414113 [01:23<00:08, 4470.70it/s]\u001b[A\n",
      " 91%|█████████ | 376397/414113 [01:23<00:08, 4486.78it/s]\u001b[A\n",
      " 91%|█████████ | 376846/414113 [01:23<00:08, 4472.65it/s]\u001b[A\n",
      " 91%|█████████ | 377296/414113 [01:24<00:08, 4479.59it/s]\u001b[A\n",
      " 91%|█████████ | 377750/414113 [01:24<00:08, 4497.30it/s]\u001b[A\n",
      " 91%|█████████▏| 378204/414113 [01:24<00:07, 4507.89it/s]\u001b[A\n",
      " 91%|█████████▏| 378655/414113 [01:24<00:07, 4491.89it/s]\u001b[A\n",
      " 92%|█████████▏| 379111/414113 [01:24<00:07, 4511.24it/s]\u001b[A\n",
      " 92%|█████████▏| 379563/414113 [01:24<00:07, 4502.43it/s]\u001b[A\n",
      " 92%|█████████▏| 380018/414113 [01:24<00:07, 4515.54it/s]\u001b[A\n",
      " 92%|█████████▏| 380473/414113 [01:24<00:07, 4524.78it/s]\u001b[A\n",
      " 92%|█████████▏| 380929/414113 [01:24<00:07, 4533.36it/s]\u001b[A\n",
      " 92%|█████████▏| 381389/414113 [01:24<00:07, 4551.16it/s]\u001b[A\n",
      " 92%|█████████▏| 381849/414113 [01:25<00:07, 4563.97it/s]\u001b[A\n",
      " 92%|█████████▏| 382306/414113 [01:25<00:06, 4552.12it/s]\u001b[A\n",
      " 92%|█████████▏| 382762/414113 [01:25<00:06, 4532.47it/s]\u001b[A\n",
      " 93%|█████████▎| 383225/414113 [01:25<00:06, 4559.57it/s]\u001b[A\n",
      " 93%|█████████▎| 383688/414113 [01:25<00:06, 4578.64it/s]\u001b[A\n",
      " 93%|█████████▎| 384146/414113 [01:25<00:06, 4549.46it/s]\u001b[A\n",
      " 93%|█████████▎| 384602/414113 [01:25<00:06, 4520.84it/s]\u001b[A\n",
      " 93%|█████████▎| 385059/414113 [01:25<00:06, 4534.47it/s]\u001b[A\n",
      " 93%|█████████▎| 385513/414113 [01:25<00:06, 4520.43it/s]\u001b[A\n",
      " 93%|█████████▎| 385966/414113 [01:25<00:06, 4499.86it/s]\u001b[A\n",
      " 93%|█████████▎| 386422/414113 [01:26<00:06, 4516.56it/s]\u001b[A\n",
      " 93%|█████████▎| 386874/414113 [01:26<00:06, 4471.52it/s]\u001b[A\n",
      " 94%|█████████▎| 387322/414113 [01:26<00:06, 4354.99it/s]\u001b[A\n",
      " 94%|█████████▎| 387791/414113 [01:26<00:05, 4449.73it/s]\u001b[A\n",
      " 94%|█████████▍| 388238/414113 [01:26<00:06, 3982.01it/s]\u001b[A\n",
      " 94%|█████████▍| 388683/414113 [01:26<00:06, 4110.63it/s]\u001b[A\n",
      " 94%|█████████▍| 389139/414113 [01:26<00:05, 4233.70it/s]\u001b[A\n",
      " 94%|█████████▍| 389594/414113 [01:26<00:05, 4322.86it/s]\u001b[A\n",
      " 94%|█████████▍| 390042/414113 [01:26<00:05, 4367.40it/s]\u001b[A\n",
      " 94%|█████████▍| 390500/414113 [01:27<00:05, 4427.05it/s]\u001b[A\n",
      " 94%|█████████▍| 390959/414113 [01:27<00:05, 4473.50it/s]\u001b[A\n",
      " 95%|█████████▍| 391410/414113 [01:27<00:05, 4482.69it/s]\u001b[A\n",
      " 95%|█████████▍| 391878/414113 [01:27<00:04, 4539.40it/s]\u001b[A\n",
      " 95%|█████████▍| 392334/414113 [01:27<00:04, 4527.09it/s]\u001b[A\n",
      " 95%|█████████▍| 392789/414113 [01:27<00:04, 4531.18it/s]\u001b[A\n",
      " 95%|█████████▍| 393243/414113 [01:27<00:04, 4469.81it/s]\u001b[A\n",
      " 95%|█████████▌| 393697/414113 [01:27<00:04, 4490.61it/s]\u001b[A\n",
      " 95%|█████████▌| 394152/414113 [01:27<00:04, 4506.65it/s]\u001b[A\n",
      " 95%|█████████▌| 394603/414113 [01:27<00:04, 4486.13it/s]\u001b[A\n",
      " 95%|█████████▌| 395052/414113 [01:28<00:04, 4486.70it/s]\u001b[A\n",
      " 96%|█████████▌| 395501/414113 [01:28<00:04, 4464.14it/s]\u001b[A\n",
      " 96%|█████████▌| 395959/414113 [01:28<00:04, 4498.25it/s]\u001b[A\n",
      " 96%|█████████▌| 396418/414113 [01:28<00:03, 4525.25it/s]\u001b[A\n",
      " 96%|█████████▌| 396874/414113 [01:28<00:03, 4535.01it/s]\u001b[A\n",
      " 96%|█████████▌| 397328/414113 [01:28<00:03, 4527.00it/s]\u001b[A\n",
      " 96%|█████████▌| 397784/414113 [01:28<00:03, 4535.58it/s]\u001b[A\n",
      " 96%|█████████▌| 398246/414113 [01:28<00:03, 4558.93it/s]\u001b[A\n",
      " 96%|█████████▋| 398704/414113 [01:28<00:03, 4562.23it/s]\u001b[A\n",
      " 96%|█████████▋| 399164/414113 [01:28<00:03, 4571.27it/s]\u001b[A\n",
      " 97%|█████████▋| 399622/414113 [01:29<00:03, 4558.23it/s]\u001b[A\n",
      " 97%|█████████▋| 400079/414113 [01:29<00:03, 4559.09it/s]\u001b[A\n",
      " 97%|█████████▋| 400535/414113 [01:29<00:02, 4549.97it/s]\u001b[A\n",
      " 97%|█████████▋| 400994/414113 [01:29<00:02, 4560.64it/s]\u001b[A\n",
      " 97%|█████████▋| 401453/414113 [01:29<00:02, 4568.07it/s]\u001b[A\n",
      " 97%|█████████▋| 401916/414113 [01:29<00:02, 4582.03it/s]\u001b[A\n",
      " 97%|█████████▋| 402375/414113 [01:29<00:02, 4575.75it/s]\u001b[A\n",
      " 97%|█████████▋| 402833/414113 [01:29<00:02, 4568.57it/s]\u001b[A\n",
      " 97%|█████████▋| 403290/414113 [01:29<00:02, 4567.08it/s]\u001b[A\n",
      " 97%|█████████▋| 403747/414113 [01:29<00:02, 4541.98it/s]\u001b[A\n",
      " 98%|█████████▊| 404209/414113 [01:30<00:02, 4564.18it/s]\u001b[A\n",
      " 98%|█████████▊| 404670/414113 [01:30<00:02, 4574.74it/s]\u001b[A\n",
      " 98%|█████████▊| 405128/414113 [01:30<00:01, 4527.11it/s]\u001b[A\n",
      " 98%|█████████▊| 405591/414113 [01:30<00:01, 4557.33it/s]\u001b[A\n",
      " 98%|█████████▊| 406047/414113 [01:30<00:01, 4530.02it/s]\u001b[A\n",
      " 98%|█████████▊| 406505/414113 [01:30<00:01, 4543.71it/s]\u001b[A\n",
      " 98%|█████████▊| 406960/414113 [01:30<00:01, 4540.37it/s]\u001b[A\n",
      " 98%|█████████▊| 407415/414113 [01:30<00:01, 4499.38it/s]\u001b[A\n",
      " 98%|█████████▊| 407879/414113 [01:30<00:01, 4539.71it/s]\u001b[A\n",
      " 99%|█████████▊| 408337/414113 [01:30<00:01, 4550.28it/s]\u001b[A\n",
      " 99%|█████████▊| 408798/414113 [01:31<00:01, 4567.87it/s]\u001b[A\n",
      " 99%|█████████▉| 409259/414113 [01:31<00:01, 4578.79it/s]\u001b[A\n",
      " 99%|█████████▉| 409722/414113 [01:31<00:00, 4592.66it/s]\u001b[A\n",
      " 99%|█████████▉| 410182/414113 [01:31<00:00, 4587.16it/s]\u001b[A\n",
      " 99%|█████████▉| 410643/414113 [01:31<00:00, 4592.71it/s]\u001b[A\n",
      " 99%|█████████▉| 411104/414113 [01:31<00:00, 4596.90it/s]\u001b[A\n",
      " 99%|█████████▉| 411572/414113 [01:31<00:00, 4619.67it/s]\u001b[A\n",
      " 99%|█████████▉| 412035/414113 [01:31<00:00, 4561.59it/s]\u001b[A\n",
      "100%|█████████▉| 412503/414113 [01:31<00:00, 4593.91it/s]\u001b[A\n",
      "100%|█████████▉| 412963/414113 [01:31<00:00, 4590.32it/s]\u001b[A\n",
      "100%|█████████▉| 413423/414113 [01:32<00:00, 4545.38it/s]\u001b[A\n",
      "100%|█████████▉| 413878/414113 [01:32<00:00, 4544.19it/s]\u001b[A\n",
      "100%|██████████| 414113/414113 [01:32<00:00, 4490.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 16          # batch size\n",
    "vocab_threshold = 4        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/25883], Loss: 3.9981, Perplexity: 54.4934\n",
      "Epoch [1/3], Step [200/25883], Loss: 4.0755, Perplexity: 58.88259\n",
      "Epoch [1/3], Step [300/25883], Loss: 3.4486, Perplexity: 31.4562\n",
      "Epoch [1/3], Step [400/25883], Loss: 4.0669, Perplexity: 58.37824\n",
      "Epoch [1/3], Step [500/25883], Loss: 3.9815, Perplexity: 53.6000\n",
      "Epoch [1/3], Step [600/25883], Loss: 3.3980, Perplexity: 29.9031\n",
      "Epoch [1/3], Step [700/25883], Loss: 3.2095, Perplexity: 24.76735\n",
      "Epoch [1/3], Step [800/25883], Loss: 3.7643, Perplexity: 43.1339\n",
      "Epoch [1/3], Step [900/25883], Loss: 3.0798, Perplexity: 21.7537\n",
      "Epoch [1/3], Step [1000/25883], Loss: 3.3768, Perplexity: 29.2780\n",
      "Epoch [1/3], Step [1100/25883], Loss: 3.0386, Perplexity: 20.8765\n",
      "Epoch [1/3], Step [1200/25883], Loss: 2.4800, Perplexity: 11.9416\n",
      "Epoch [1/3], Step [1300/25883], Loss: 3.0340, Perplexity: 20.7806\n",
      "Epoch [1/3], Step [1400/25883], Loss: 2.8158, Perplexity: 16.7062\n",
      "Epoch [1/3], Step [1500/25883], Loss: 3.4940, Perplexity: 32.9174\n",
      "Epoch [1/3], Step [1600/25883], Loss: 3.1521, Perplexity: 23.3850\n",
      "Epoch [1/3], Step [1700/25883], Loss: 3.4024, Perplexity: 30.0362\n",
      "Epoch [1/3], Step [1800/25883], Loss: 3.4582, Perplexity: 31.7599\n",
      "Epoch [1/3], Step [1900/25883], Loss: 2.4629, Perplexity: 11.7388\n",
      "Epoch [1/3], Step [2000/25883], Loss: 3.3461, Perplexity: 28.3904\n",
      "Epoch [1/3], Step [2100/25883], Loss: 2.7217, Perplexity: 15.2057\n",
      "Epoch [1/3], Step [2200/25883], Loss: 2.3421, Perplexity: 10.4029\n",
      "Epoch [1/3], Step [2300/25883], Loss: 2.9998, Perplexity: 20.0822\n",
      "Epoch [1/3], Step [2400/25883], Loss: 2.7597, Perplexity: 15.7956\n",
      "Epoch [1/3], Step [2500/25883], Loss: 2.2091, Perplexity: 9.10753\n",
      "Epoch [1/3], Step [2600/25883], Loss: 2.6880, Perplexity: 14.7017\n",
      "Epoch [1/3], Step [2700/25883], Loss: 2.6080, Perplexity: 13.5714\n",
      "Epoch [1/3], Step [2800/25883], Loss: 3.4232, Perplexity: 30.6688\n",
      "Epoch [1/3], Step [2900/25883], Loss: 3.1828, Perplexity: 24.1135\n",
      "Epoch [1/3], Step [3000/25883], Loss: 3.0544, Perplexity: 21.2091\n",
      "Epoch [1/3], Step [3100/25883], Loss: 2.5465, Perplexity: 12.7625\n",
      "Epoch [1/3], Step [3200/25883], Loss: 2.8204, Perplexity: 16.7843\n",
      "Epoch [1/3], Step [3300/25883], Loss: 3.2147, Perplexity: 24.8948\n",
      "Epoch [1/3], Step [3400/25883], Loss: 3.1884, Perplexity: 24.2497\n",
      "Epoch [1/3], Step [3500/25883], Loss: 2.7839, Perplexity: 16.1818\n",
      "Epoch [1/3], Step [3600/25883], Loss: 2.9103, Perplexity: 18.3615\n",
      "Epoch [1/3], Step [3700/25883], Loss: 2.7663, Perplexity: 15.8994\n",
      "Epoch [1/3], Step [3800/25883], Loss: 2.5146, Perplexity: 12.36181\n",
      "Epoch [1/3], Step [3900/25883], Loss: 2.8604, Perplexity: 17.4690\n",
      "Epoch [1/3], Step [4000/25883], Loss: 3.4534, Perplexity: 31.6086\n",
      "Epoch [1/3], Step [4100/25883], Loss: 2.6739, Perplexity: 14.4970\n",
      "Epoch [1/3], Step [4200/25883], Loss: 2.7371, Perplexity: 15.4425\n",
      "Epoch [1/3], Step [4300/25883], Loss: 2.5880, Perplexity: 13.3034\n",
      "Epoch [1/3], Step [4400/25883], Loss: 2.6549, Perplexity: 14.2238\n",
      "Epoch [1/3], Step [4500/25883], Loss: 3.5429, Perplexity: 34.5669\n",
      "Epoch [1/3], Step [4600/25883], Loss: 3.0350, Perplexity: 20.8006\n",
      "Epoch [1/3], Step [4700/25883], Loss: 2.7732, Perplexity: 16.0099\n",
      "Epoch [1/3], Step [4800/25883], Loss: 2.6009, Perplexity: 13.4764\n",
      "Epoch [1/3], Step [4900/25883], Loss: 2.6858, Perplexity: 14.6700\n",
      "Epoch [1/3], Step [5000/25883], Loss: 2.4435, Perplexity: 11.5137\n",
      "Epoch [1/3], Step [5100/25883], Loss: 2.4912, Perplexity: 12.0754\n",
      "Epoch [1/3], Step [5200/25883], Loss: 2.8749, Perplexity: 17.72386\n",
      "Epoch [1/3], Step [5300/25883], Loss: 2.3893, Perplexity: 10.9058\n",
      "Epoch [1/3], Step [5400/25883], Loss: 2.4721, Perplexity: 11.8473\n",
      "Epoch [1/3], Step [5500/25883], Loss: 2.8106, Perplexity: 16.6191\n",
      "Epoch [1/3], Step [5600/25883], Loss: 2.2870, Perplexity: 9.84565\n",
      "Epoch [1/3], Step [5700/25883], Loss: 2.9410, Perplexity: 18.9345\n",
      "Epoch [1/3], Step [5800/25883], Loss: 2.4861, Perplexity: 12.0147\n",
      "Epoch [1/3], Step [5900/25883], Loss: 2.6267, Perplexity: 13.8282\n",
      "Epoch [1/3], Step [6000/25883], Loss: 2.1591, Perplexity: 8.66374\n",
      "Epoch [1/3], Step [6100/25883], Loss: 2.9591, Perplexity: 19.2814\n",
      "Epoch [1/3], Step [6200/25883], Loss: 2.2578, Perplexity: 9.56181\n",
      "Epoch [1/3], Step [6300/25883], Loss: 2.4972, Perplexity: 12.1485\n",
      "Epoch [1/3], Step [6400/25883], Loss: 3.0814, Perplexity: 21.7885\n",
      "Epoch [1/3], Step [6500/25883], Loss: 2.4097, Perplexity: 11.1301\n",
      "Epoch [1/3], Step [6600/25883], Loss: 3.2713, Perplexity: 26.3448\n",
      "Epoch [1/3], Step [6700/25883], Loss: 2.4061, Perplexity: 11.0907\n",
      "Epoch [1/3], Step [6800/25883], Loss: 2.9169, Perplexity: 18.4832\n",
      "Epoch [1/3], Step [6900/25883], Loss: 2.6736, Perplexity: 14.4927\n",
      "Epoch [1/3], Step [7000/25883], Loss: 2.4479, Perplexity: 11.5638\n",
      "Epoch [1/3], Step [7100/25883], Loss: 3.0061, Perplexity: 20.2083\n",
      "Epoch [1/3], Step [7200/25883], Loss: 2.7544, Perplexity: 15.7120\n",
      "Epoch [1/3], Step [7300/25883], Loss: 2.6807, Perplexity: 14.5958\n",
      "Epoch [1/3], Step [7400/25883], Loss: 2.3447, Perplexity: 10.4305\n",
      "Epoch [1/3], Step [7500/25883], Loss: 2.4581, Perplexity: 11.6824\n",
      "Epoch [1/3], Step [7600/25883], Loss: 2.9305, Perplexity: 18.7368\n",
      "Epoch [1/3], Step [7700/25883], Loss: 2.3632, Perplexity: 10.6248\n",
      "Epoch [1/3], Step [7800/25883], Loss: 2.4098, Perplexity: 11.1318\n",
      "Epoch [1/3], Step [7900/25883], Loss: 2.2758, Perplexity: 9.73623\n",
      "Epoch [1/3], Step [8000/25883], Loss: 2.8384, Perplexity: 17.0888\n",
      "Epoch [1/3], Step [8100/25883], Loss: 2.3345, Perplexity: 10.3245\n",
      "Epoch [1/3], Step [8200/25883], Loss: 2.4599, Perplexity: 11.7031\n",
      "Epoch [1/3], Step [8300/25883], Loss: 2.5541, Perplexity: 12.8601\n",
      "Epoch [1/3], Step [8400/25883], Loss: 2.6540, Perplexity: 14.2110\n",
      "Epoch [1/3], Step [8500/25883], Loss: 2.6446, Perplexity: 14.0775\n",
      "Epoch [1/3], Step [8600/25883], Loss: 2.8279, Perplexity: 16.9093\n",
      "Epoch [1/3], Step [8700/25883], Loss: 2.1763, Perplexity: 8.81367\n",
      "Epoch [1/3], Step [8800/25883], Loss: 2.6830, Perplexity: 14.62958\n",
      "Epoch [1/3], Step [8900/25883], Loss: 2.1956, Perplexity: 8.98545\n",
      "Epoch [1/3], Step [9000/25883], Loss: 2.5069, Perplexity: 12.2663\n",
      "Epoch [1/3], Step [9100/25883], Loss: 2.8272, Perplexity: 16.8981\n",
      "Epoch [1/3], Step [9200/25883], Loss: 2.4126, Perplexity: 11.1625\n",
      "Epoch [1/3], Step [9300/25883], Loss: 2.2827, Perplexity: 9.80345\n",
      "Epoch [1/3], Step [9400/25883], Loss: 2.6465, Perplexity: 14.1045\n",
      "Epoch [1/3], Step [9500/25883], Loss: 2.5058, Perplexity: 12.2534\n",
      "Epoch [1/3], Step [9600/25883], Loss: 2.8216, Perplexity: 16.8035\n",
      "Epoch [1/3], Step [9700/25883], Loss: 2.8618, Perplexity: 17.49343\n",
      "Epoch [1/3], Step [9800/25883], Loss: 2.4121, Perplexity: 11.1568\n",
      "Epoch [1/3], Step [9900/25883], Loss: 3.0852, Perplexity: 21.8726\n",
      "Epoch [1/3], Step [10000/25883], Loss: 2.4990, Perplexity: 12.1707\n",
      "Epoch [1/3], Step [10100/25883], Loss: 2.1989, Perplexity: 9.01556\n",
      "Epoch [1/3], Step [10200/25883], Loss: 2.1848, Perplexity: 8.88900\n",
      "Epoch [1/3], Step [10300/25883], Loss: 2.2159, Perplexity: 9.16960\n",
      "Epoch [1/3], Step [10400/25883], Loss: 2.4337, Perplexity: 11.4010\n",
      "Epoch [1/3], Step [10500/25883], Loss: 2.6990, Perplexity: 14.8653\n",
      "Epoch [1/3], Step [10600/25883], Loss: 2.5948, Perplexity: 13.3937\n",
      "Epoch [1/3], Step [10700/25883], Loss: 2.7396, Perplexity: 15.4809\n",
      "Epoch [1/3], Step [10800/25883], Loss: 2.2067, Perplexity: 9.08548\n",
      "Epoch [1/3], Step [10900/25883], Loss: 1.9766, Perplexity: 7.21827\n",
      "Epoch [1/3], Step [11000/25883], Loss: 2.7270, Perplexity: 15.2871\n",
      "Epoch [1/3], Step [11100/25883], Loss: 2.3662, Perplexity: 10.6569\n",
      "Epoch [1/3], Step [11200/25883], Loss: 2.9533, Perplexity: 19.1697\n",
      "Epoch [1/3], Step [11300/25883], Loss: 2.1018, Perplexity: 8.18056\n",
      "Epoch [1/3], Step [11400/25883], Loss: 2.3155, Perplexity: 10.1302\n",
      "Epoch [1/3], Step [11500/25883], Loss: 2.2655, Perplexity: 9.63623\n",
      "Epoch [1/3], Step [11600/25883], Loss: 2.1914, Perplexity: 8.94794\n",
      "Epoch [1/3], Step [11700/25883], Loss: 2.7374, Perplexity: 15.44637\n",
      "Epoch [1/3], Step [11800/25883], Loss: 2.5184, Perplexity: 12.4093\n",
      "Epoch [1/3], Step [11900/25883], Loss: 2.1995, Perplexity: 9.02015\n",
      "Epoch [1/3], Step [12000/25883], Loss: 2.3483, Perplexity: 10.4681\n",
      "Epoch [1/3], Step [12100/25883], Loss: 2.7791, Perplexity: 16.1041\n",
      "Epoch [1/3], Step [12200/25883], Loss: 2.8262, Perplexity: 16.8813\n",
      "Epoch [1/3], Step [12300/25883], Loss: 2.8722, Perplexity: 17.6755\n",
      "Epoch [1/3], Step [12400/25883], Loss: 2.2925, Perplexity: 9.89954\n",
      "Epoch [1/3], Step [12500/25883], Loss: 2.4369, Perplexity: 11.4371\n",
      "Epoch [1/3], Step [12600/25883], Loss: 2.5812, Perplexity: 13.2130\n",
      "Epoch [1/3], Step [12700/25883], Loss: 2.2131, Perplexity: 9.14441\n",
      "Epoch [1/3], Step [12800/25883], Loss: 2.3556, Perplexity: 10.5448\n",
      "Epoch [1/3], Step [12900/25883], Loss: 2.6204, Perplexity: 13.7412\n",
      "Epoch [1/3], Step [13000/25883], Loss: 2.3483, Perplexity: 10.4677\n",
      "Epoch [1/3], Step [13100/25883], Loss: 2.2121, Perplexity: 9.13531\n",
      "Epoch [1/3], Step [13200/25883], Loss: 1.9428, Perplexity: 6.97818\n",
      "Epoch [1/3], Step [13300/25883], Loss: 2.1074, Perplexity: 8.22660\n",
      "Epoch [1/3], Step [13400/25883], Loss: 2.5351, Perplexity: 12.6177\n",
      "Epoch [1/3], Step [13500/25883], Loss: 2.5389, Perplexity: 12.6659\n",
      "Epoch [1/3], Step [13600/25883], Loss: 2.9638, Perplexity: 19.3723\n",
      "Epoch [1/3], Step [13700/25883], Loss: 2.7675, Perplexity: 15.9190\n",
      "Epoch [1/3], Step [13800/25883], Loss: 2.8097, Perplexity: 16.6044\n",
      "Epoch [1/3], Step [13900/25883], Loss: 1.9600, Perplexity: 7.09958\n",
      "Epoch [1/3], Step [14000/25883], Loss: 2.0024, Perplexity: 7.40657\n",
      "Epoch [1/3], Step [14100/25883], Loss: 2.1041, Perplexity: 8.20012\n",
      "Epoch [1/3], Step [14200/25883], Loss: 2.0056, Perplexity: 7.43069\n",
      "Epoch [1/3], Step [14300/25883], Loss: 2.4162, Perplexity: 11.2034\n",
      "Epoch [1/3], Step [14400/25883], Loss: 2.3221, Perplexity: 10.1967\n",
      "Epoch [1/3], Step [14500/25883], Loss: 2.3793, Perplexity: 10.7973\n",
      "Epoch [1/3], Step [14600/25883], Loss: 2.9015, Perplexity: 18.2007\n",
      "Epoch [1/3], Step [14700/25883], Loss: 2.2309, Perplexity: 9.308765\n",
      "Epoch [1/3], Step [14800/25883], Loss: 2.6570, Perplexity: 14.2540\n",
      "Epoch [1/3], Step [14900/25883], Loss: 2.3480, Perplexity: 10.46503\n",
      "Epoch [1/3], Step [15000/25883], Loss: 2.8981, Perplexity: 18.1396\n",
      "Epoch [1/3], Step [15100/25883], Loss: 2.0624, Perplexity: 7.864862\n",
      "Epoch [1/3], Step [15200/25883], Loss: 2.7044, Perplexity: 14.9449\n",
      "Epoch [1/3], Step [15300/25883], Loss: 2.0265, Perplexity: 7.58722\n",
      "Epoch [1/3], Step [15400/25883], Loss: 2.1940, Perplexity: 8.97087\n",
      "Epoch [1/3], Step [15500/25883], Loss: 3.2344, Perplexity: 25.3910\n",
      "Epoch [1/3], Step [15600/25883], Loss: 2.6085, Perplexity: 13.5790\n",
      "Epoch [1/3], Step [15700/25883], Loss: 2.6120, Perplexity: 13.6268\n",
      "Epoch [1/3], Step [15800/25883], Loss: 2.4792, Perplexity: 11.9315\n",
      "Epoch [1/3], Step [15900/25883], Loss: 2.3399, Perplexity: 10.3799\n",
      "Epoch [1/3], Step [16000/25883], Loss: 2.4832, Perplexity: 11.9796\n",
      "Epoch [1/3], Step [16100/25883], Loss: 2.7028, Perplexity: 14.9216\n",
      "Epoch [1/3], Step [16200/25883], Loss: 1.9646, Perplexity: 7.13199\n",
      "Epoch [1/3], Step [16300/25883], Loss: 2.2024, Perplexity: 9.04663\n",
      "Epoch [1/3], Step [16400/25883], Loss: 2.4196, Perplexity: 11.2419\n",
      "Epoch [1/3], Step [16500/25883], Loss: 2.2997, Perplexity: 9.97114\n",
      "Epoch [1/3], Step [16600/25883], Loss: 2.2697, Perplexity: 9.67645\n",
      "Epoch [1/3], Step [16700/25883], Loss: 2.3728, Perplexity: 10.7277\n",
      "Epoch [1/3], Step [16800/25883], Loss: 2.3181, Perplexity: 10.1563\n",
      "Epoch [1/3], Step [16900/25883], Loss: 2.5186, Perplexity: 12.4117\n",
      "Epoch [1/3], Step [17000/25883], Loss: 2.1919, Perplexity: 8.95240\n",
      "Epoch [1/3], Step [17100/25883], Loss: 2.2876, Perplexity: 9.85130\n",
      "Epoch [1/3], Step [17200/25883], Loss: 2.2550, Perplexity: 9.53514\n",
      "Epoch [1/3], Step [17300/25883], Loss: 2.5459, Perplexity: 12.7543\n",
      "Epoch [1/3], Step [17400/25883], Loss: 2.4797, Perplexity: 11.9374\n",
      "Epoch [1/3], Step [17500/25883], Loss: 2.3758, Perplexity: 10.7598\n",
      "Epoch [1/3], Step [17600/25883], Loss: 2.0626, Perplexity: 7.86643\n",
      "Epoch [1/3], Step [17700/25883], Loss: 2.3030, Perplexity: 10.0042\n",
      "Epoch [1/3], Step [17800/25883], Loss: 2.6129, Perplexity: 13.6384\n",
      "Epoch [1/3], Step [17900/25883], Loss: 2.5020, Perplexity: 12.2073\n",
      "Epoch [1/3], Step [18000/25883], Loss: 2.6622, Perplexity: 14.3273\n",
      "Epoch [1/3], Step [18100/25883], Loss: 2.8170, Perplexity: 16.7268\n",
      "Epoch [1/3], Step [18200/25883], Loss: 2.3131, Perplexity: 10.1056\n",
      "Epoch [1/3], Step [18300/25883], Loss: 2.7979, Perplexity: 16.4103\n",
      "Epoch [1/3], Step [18400/25883], Loss: 2.4951, Perplexity: 12.1225\n",
      "Epoch [1/3], Step [18500/25883], Loss: 2.4806, Perplexity: 11.9486\n",
      "Epoch [1/3], Step [18600/25883], Loss: 2.2626, Perplexity: 9.60776\n",
      "Epoch [1/3], Step [18700/25883], Loss: 3.0289, Perplexity: 20.6745\n",
      "Epoch [1/3], Step [18800/25883], Loss: 2.3723, Perplexity: 10.7222\n",
      "Epoch [1/3], Step [18900/25883], Loss: 2.7148, Perplexity: 15.1010\n",
      "Epoch [1/3], Step [19000/25883], Loss: 2.6758, Perplexity: 14.5237\n",
      "Epoch [1/3], Step [19100/25883], Loss: 2.4804, Perplexity: 11.9455\n",
      "Epoch [1/3], Step [19200/25883], Loss: 2.6104, Perplexity: 13.6049\n",
      "Epoch [1/3], Step [19300/25883], Loss: 2.3501, Perplexity: 10.4861\n",
      "Epoch [1/3], Step [19400/25883], Loss: 2.7386, Perplexity: 15.4652\n",
      "Epoch [1/3], Step [19500/25883], Loss: 2.1163, Perplexity: 8.300329\n",
      "Epoch [1/3], Step [19600/25883], Loss: 2.3067, Perplexity: 10.0411\n",
      "Epoch [1/3], Step [19700/25883], Loss: 2.3273, Perplexity: 10.2498\n",
      "Epoch [1/3], Step [19800/25883], Loss: 2.2605, Perplexity: 9.58827\n",
      "Epoch [1/3], Step [19900/25883], Loss: 1.9629, Perplexity: 7.12026\n",
      "Epoch [1/3], Step [20000/25883], Loss: 2.0899, Perplexity: 8.08434\n",
      "Epoch [1/3], Step [20100/25883], Loss: 2.3461, Perplexity: 10.4452\n",
      "Epoch [1/3], Step [20200/25883], Loss: 2.5372, Perplexity: 12.6446\n",
      "Epoch [1/3], Step [20300/25883], Loss: 2.3604, Perplexity: 10.5955\n",
      "Epoch [1/3], Step [20400/25883], Loss: 2.2875, Perplexity: 9.85058\n",
      "Epoch [1/3], Step [20500/25883], Loss: 3.0322, Perplexity: 20.7432\n",
      "Epoch [1/3], Step [20600/25883], Loss: 2.1936, Perplexity: 8.96711\n",
      "Epoch [1/3], Step [20700/25883], Loss: 2.5657, Perplexity: 13.0104\n",
      "Epoch [1/3], Step [20800/25883], Loss: 2.6475, Perplexity: 14.1184\n",
      "Epoch [1/3], Step [20900/25883], Loss: 2.5600, Perplexity: 12.9364\n",
      "Epoch [1/3], Step [21000/25883], Loss: 2.5217, Perplexity: 12.4503\n",
      "Epoch [1/3], Step [21100/25883], Loss: 2.2714, Perplexity: 9.69288\n",
      "Epoch [1/3], Step [21200/25883], Loss: 2.4328, Perplexity: 11.3903\n",
      "Epoch [1/3], Step [21300/25883], Loss: 2.8409, Perplexity: 17.1305\n",
      "Epoch [1/3], Step [21400/25883], Loss: 2.5815, Perplexity: 13.2166\n",
      "Epoch [1/3], Step [21500/25883], Loss: 2.0296, Perplexity: 7.61114\n",
      "Epoch [1/3], Step [21600/25883], Loss: 2.3627, Perplexity: 10.6197\n",
      "Epoch [1/3], Step [21700/25883], Loss: 2.1568, Perplexity: 8.64346\n",
      "Epoch [1/3], Step [21800/25883], Loss: 2.4380, Perplexity: 11.4505\n",
      "Epoch [1/3], Step [21900/25883], Loss: 2.4995, Perplexity: 12.1769\n",
      "Epoch [1/3], Step [22000/25883], Loss: 2.6331, Perplexity: 13.9174\n",
      "Epoch [1/3], Step [22100/25883], Loss: 2.5568, Perplexity: 12.8946\n",
      "Epoch [1/3], Step [22200/25883], Loss: 2.8932, Perplexity: 18.0501\n",
      "Epoch [1/3], Step [22300/25883], Loss: 2.2987, Perplexity: 9.96131\n",
      "Epoch [1/3], Step [22400/25883], Loss: 2.1805, Perplexity: 8.85061\n",
      "Epoch [1/3], Step [22500/25883], Loss: 2.3264, Perplexity: 10.2411\n",
      "Epoch [1/3], Step [22600/25883], Loss: 2.6561, Perplexity: 14.2405\n",
      "Epoch [1/3], Step [22700/25883], Loss: 2.0932, Perplexity: 8.11078\n",
      "Epoch [1/3], Step [22800/25883], Loss: 2.8807, Perplexity: 17.8268\n",
      "Epoch [1/3], Step [22900/25883], Loss: 3.0315, Perplexity: 20.7290\n",
      "Epoch [1/3], Step [23000/25883], Loss: 2.1148, Perplexity: 8.28792\n",
      "Epoch [1/3], Step [23100/25883], Loss: 2.5675, Perplexity: 13.0336\n",
      "Epoch [1/3], Step [23200/25883], Loss: 2.4260, Perplexity: 11.3137\n",
      "Epoch [1/3], Step [23300/25883], Loss: 2.3485, Perplexity: 10.4703\n",
      "Epoch [1/3], Step [23400/25883], Loss: 2.0202, Perplexity: 7.53976\n",
      "Epoch [1/3], Step [23500/25883], Loss: 2.6670, Perplexity: 14.3966\n",
      "Epoch [1/3], Step [23600/25883], Loss: 2.4167, Perplexity: 11.2093\n",
      "Epoch [1/3], Step [23700/25883], Loss: 2.0845, Perplexity: 8.04082\n",
      "Epoch [1/3], Step [23800/25883], Loss: 2.4283, Perplexity: 11.3396\n",
      "Epoch [1/3], Step [23900/25883], Loss: 2.3197, Perplexity: 10.1731\n",
      "Epoch [1/3], Step [24000/25883], Loss: 2.3574, Perplexity: 10.5636\n",
      "Epoch [1/3], Step [24100/25883], Loss: 2.1168, Perplexity: 8.30428\n",
      "Epoch [1/3], Step [24200/25883], Loss: 2.4532, Perplexity: 11.6258\n",
      "Epoch [1/3], Step [24300/25883], Loss: 2.2742, Perplexity: 9.72062\n",
      "Epoch [1/3], Step [24400/25883], Loss: 2.7683, Perplexity: 15.9311\n",
      "Epoch [1/3], Step [24500/25883], Loss: 2.4564, Perplexity: 11.6630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/25883], Loss: 2.3600, Perplexity: 10.5912\n",
      "Epoch [1/3], Step [24700/25883], Loss: 2.4026, Perplexity: 11.0518\n",
      "Epoch [1/3], Step [24800/25883], Loss: 1.9041, Perplexity: 6.71318\n",
      "Epoch [1/3], Step [24900/25883], Loss: 2.2888, Perplexity: 9.86290\n",
      "Epoch [1/3], Step [25000/25883], Loss: 1.9155, Perplexity: 6.79049\n",
      "Epoch [1/3], Step [25100/25883], Loss: 2.3085, Perplexity: 10.0594\n",
      "Epoch [1/3], Step [25200/25883], Loss: 2.2429, Perplexity: 9.42042\n",
      "Epoch [1/3], Step [25300/25883], Loss: 2.5237, Perplexity: 12.4747\n",
      "Epoch [1/3], Step [25400/25883], Loss: 2.3577, Perplexity: 10.5665\n",
      "Epoch [1/3], Step [25500/25883], Loss: 2.7412, Perplexity: 15.5055\n",
      "Epoch [1/3], Step [25600/25883], Loss: 2.1635, Perplexity: 8.70153\n",
      "Epoch [1/3], Step [25700/25883], Loss: 2.1220, Perplexity: 8.34764\n",
      "Epoch [1/3], Step [25800/25883], Loss: 2.5100, Perplexity: 12.3047\n",
      "Epoch [2/3], Step [100/25883], Loss: 1.6905, Perplexity: 5.4224202\n",
      "Epoch [2/3], Step [200/25883], Loss: 2.1608, Perplexity: 8.67773\n",
      "Epoch [2/3], Step [300/25883], Loss: 1.9165, Perplexity: 6.79724\n",
      "Epoch [2/3], Step [400/25883], Loss: 2.4479, Perplexity: 11.5641\n",
      "Epoch [2/3], Step [500/25883], Loss: 2.4070, Perplexity: 11.1003\n",
      "Epoch [2/3], Step [600/25883], Loss: 2.6620, Perplexity: 14.3247\n",
      "Epoch [2/3], Step [700/25883], Loss: 2.5007, Perplexity: 12.1907\n",
      "Epoch [2/3], Step [800/25883], Loss: 2.2649, Perplexity: 9.62976\n",
      "Epoch [2/3], Step [900/25883], Loss: 2.2290, Perplexity: 9.29108\n",
      "Epoch [2/3], Step [1000/25883], Loss: 2.3146, Perplexity: 10.1209\n",
      "Epoch [2/3], Step [1100/25883], Loss: 2.5261, Perplexity: 12.5043\n",
      "Epoch [2/3], Step [1200/25883], Loss: 2.0555, Perplexity: 7.81056\n",
      "Epoch [2/3], Step [1300/25883], Loss: 2.5281, Perplexity: 12.5302\n",
      "Epoch [2/3], Step [1400/25883], Loss: 2.3840, Perplexity: 10.8481\n",
      "Epoch [2/3], Step [1500/25883], Loss: 2.1963, Perplexity: 8.99188\n",
      "Epoch [2/3], Step [1600/25883], Loss: 2.5098, Perplexity: 12.3025\n",
      "Epoch [2/3], Step [1700/25883], Loss: 2.7275, Perplexity: 15.2951\n",
      "Epoch [2/3], Step [1800/25883], Loss: 1.9804, Perplexity: 7.24567\n",
      "Epoch [2/3], Step [1900/25883], Loss: 2.4459, Perplexity: 11.5409\n",
      "Epoch [2/3], Step [2000/25883], Loss: 2.8347, Perplexity: 17.0252\n",
      "Epoch [2/3], Step [2100/25883], Loss: 2.6594, Perplexity: 14.2878\n",
      "Epoch [2/3], Step [2200/25883], Loss: 2.2595, Perplexity: 9.57800\n",
      "Epoch [2/3], Step [2300/25883], Loss: 1.9173, Perplexity: 6.80264\n",
      "Epoch [2/3], Step [2400/25883], Loss: 2.6883, Perplexity: 14.7069\n",
      "Epoch [2/3], Step [2500/25883], Loss: 2.1476, Perplexity: 8.56432\n",
      "Epoch [2/3], Step [2600/25883], Loss: 2.5799, Perplexity: 13.1962\n",
      "Epoch [2/3], Step [2700/25883], Loss: 2.4492, Perplexity: 11.5788\n",
      "Epoch [2/3], Step [2800/25883], Loss: 2.1184, Perplexity: 8.31787\n",
      "Epoch [2/3], Step [2900/25883], Loss: 2.2931, Perplexity: 9.90558\n",
      "Epoch [2/3], Step [3000/25883], Loss: 2.2808, Perplexity: 9.78415\n",
      "Epoch [2/3], Step [3100/25883], Loss: 2.3594, Perplexity: 10.5850\n",
      "Epoch [2/3], Step [3200/25883], Loss: 2.2771, Perplexity: 9.74855\n",
      "Epoch [2/3], Step [3300/25883], Loss: 2.8258, Perplexity: 16.8738\n",
      "Epoch [2/3], Step [3400/25883], Loss: 2.2415, Perplexity: 9.40776\n",
      "Epoch [2/3], Step [3500/25883], Loss: 2.8415, Perplexity: 17.1418\n",
      "Epoch [2/3], Step [3600/25883], Loss: 2.3695, Perplexity: 10.6926\n",
      "Epoch [2/3], Step [3700/25883], Loss: 2.2542, Perplexity: 9.52748\n",
      "Epoch [2/3], Step [3800/25883], Loss: 2.1854, Perplexity: 8.89452\n",
      "Epoch [2/3], Step [3900/25883], Loss: 2.5267, Perplexity: 12.5120\n",
      "Epoch [2/3], Step [4000/25883], Loss: 2.5932, Perplexity: 13.3727\n",
      "Epoch [2/3], Step [4100/25883], Loss: 2.2844, Perplexity: 9.81947\n",
      "Epoch [2/3], Step [4200/25883], Loss: 2.6701, Perplexity: 14.4419\n",
      "Epoch [2/3], Step [4300/25883], Loss: 2.5701, Perplexity: 13.0669\n",
      "Epoch [2/3], Step [4400/25883], Loss: 2.1250, Perplexity: 8.37265\n",
      "Epoch [2/3], Step [4500/25883], Loss: 2.1680, Perplexity: 8.74058\n",
      "Epoch [2/3], Step [4600/25883], Loss: 2.2475, Perplexity: 9.46425\n",
      "Epoch [2/3], Step [4700/25883], Loss: 2.4270, Perplexity: 11.3252\n",
      "Epoch [2/3], Step [4800/25883], Loss: 2.5001, Perplexity: 12.1832\n",
      "Epoch [2/3], Step [4900/25883], Loss: 1.9961, Perplexity: 7.36026\n",
      "Epoch [2/3], Step [5000/25883], Loss: 2.5727, Perplexity: 13.1017\n",
      "Epoch [2/3], Step [5100/25883], Loss: 2.3568, Perplexity: 10.5570\n",
      "Epoch [2/3], Step [5200/25883], Loss: 2.0240, Perplexity: 7.56831\n",
      "Epoch [2/3], Step [5300/25883], Loss: 2.9935, Perplexity: 19.9558\n",
      "Epoch [2/3], Step [5400/25883], Loss: 2.1194, Perplexity: 8.32639\n",
      "Epoch [2/3], Step [5500/25883], Loss: 2.7543, Perplexity: 15.7105\n",
      "Epoch [2/3], Step [5600/25883], Loss: 1.9948, Perplexity: 7.35055\n",
      "Epoch [2/3], Step [5700/25883], Loss: 2.2305, Perplexity: 9.30431\n",
      "Epoch [2/3], Step [5800/25883], Loss: 3.1046, Perplexity: 22.2999\n",
      "Epoch [2/3], Step [5900/25883], Loss: 2.2585, Perplexity: 9.56859\n",
      "Epoch [2/3], Step [6000/25883], Loss: 2.5610, Perplexity: 12.9489\n",
      "Epoch [2/3], Step [6100/25883], Loss: 3.1194, Perplexity: 22.6321\n",
      "Epoch [2/3], Step [6200/25883], Loss: 2.2886, Perplexity: 9.86123\n",
      "Epoch [2/3], Step [6300/25883], Loss: 2.7640, Perplexity: 15.8635\n",
      "Epoch [2/3], Step [6400/25883], Loss: 2.1486, Perplexity: 8.57296\n",
      "Epoch [2/3], Step [6500/25883], Loss: 2.1519, Perplexity: 8.60102\n",
      "Epoch [2/3], Step [6600/25883], Loss: 2.6767, Perplexity: 14.5364\n",
      "Epoch [2/3], Step [6700/25883], Loss: 2.5704, Perplexity: 13.0712\n",
      "Epoch [2/3], Step [6800/25883], Loss: 1.9749, Perplexity: 7.20619\n",
      "Epoch [2/3], Step [6900/25883], Loss: 2.2125, Perplexity: 9.138751\n",
      "Epoch [2/3], Step [7000/25883], Loss: 2.5230, Perplexity: 12.4657\n",
      "Epoch [2/3], Step [7100/25883], Loss: 2.5767, Perplexity: 13.1538\n",
      "Epoch [2/3], Step [7200/25883], Loss: 2.2851, Perplexity: 9.82693\n",
      "Epoch [2/3], Step [7300/25883], Loss: 2.1359, Perplexity: 8.464527\n",
      "Epoch [2/3], Step [7400/25883], Loss: 2.7697, Perplexity: 15.9532\n",
      "Epoch [2/3], Step [7500/25883], Loss: 1.9707, Perplexity: 7.17576\n",
      "Epoch [2/3], Step [7600/25883], Loss: 2.0117, Perplexity: 7.47599\n",
      "Epoch [2/3], Step [7700/25883], Loss: 2.3820, Perplexity: 10.8269\n",
      "Epoch [2/3], Step [7800/25883], Loss: 2.8771, Perplexity: 17.7632\n",
      "Epoch [2/3], Step [7900/25883], Loss: 2.5968, Perplexity: 13.4210\n",
      "Epoch [2/3], Step [8000/25883], Loss: 1.8485, Perplexity: 6.35052\n",
      "Epoch [2/3], Step [8100/25883], Loss: 2.4476, Perplexity: 11.5601\n",
      "Epoch [2/3], Step [8200/25883], Loss: 2.5148, Perplexity: 12.3647\n",
      "Epoch [2/3], Step [8300/25883], Loss: 2.1413, Perplexity: 8.51073\n",
      "Epoch [2/3], Step [8400/25883], Loss: 2.8189, Perplexity: 16.7591\n",
      "Epoch [2/3], Step [8500/25883], Loss: 2.2268, Perplexity: 9.27007\n",
      "Epoch [2/3], Step [8600/25883], Loss: 2.1246, Perplexity: 8.36953\n",
      "Epoch [2/3], Step [8700/25883], Loss: 2.6430, Perplexity: 14.0552\n",
      "Epoch [2/3], Step [8800/25883], Loss: 2.3042, Perplexity: 10.0159\n",
      "Epoch [2/3], Step [8900/25883], Loss: 2.2960, Perplexity: 9.93479\n",
      "Epoch [2/3], Step [9000/25883], Loss: 2.7042, Perplexity: 14.9426\n",
      "Epoch [2/3], Step [9100/25883], Loss: 2.0695, Perplexity: 7.92102\n",
      "Epoch [2/3], Step [9200/25883], Loss: 1.8771, Perplexity: 6.53484\n",
      "Epoch [2/3], Step [9300/25883], Loss: 2.2169, Perplexity: 9.17905\n",
      "Epoch [2/3], Step [9400/25883], Loss: 2.2200, Perplexity: 9.20726\n",
      "Epoch [2/3], Step [9500/25883], Loss: 2.4295, Perplexity: 11.3532\n",
      "Epoch [2/3], Step [9600/25883], Loss: 2.3870, Perplexity: 10.8811\n",
      "Epoch [2/3], Step [9700/25883], Loss: 2.2731, Perplexity: 9.70998\n",
      "Epoch [2/3], Step [9800/25883], Loss: 2.2233, Perplexity: 9.23744\n",
      "Epoch [2/3], Step [9900/25883], Loss: 2.2374, Perplexity: 9.36867\n",
      "Epoch [2/3], Step [10000/25883], Loss: 2.2684, Perplexity: 9.6638\n",
      "Epoch [2/3], Step [10100/25883], Loss: 2.2521, Perplexity: 9.50779\n",
      "Epoch [2/3], Step [10200/25883], Loss: 1.9597, Perplexity: 7.09735\n",
      "Epoch [2/3], Step [10300/25883], Loss: 2.7238, Perplexity: 15.2376\n",
      "Epoch [2/3], Step [10400/25883], Loss: 2.4913, Perplexity: 12.0770\n",
      "Epoch [2/3], Step [10500/25883], Loss: 2.5863, Perplexity: 13.2802\n",
      "Epoch [2/3], Step [10600/25883], Loss: 2.2137, Perplexity: 9.14999\n",
      "Epoch [2/3], Step [10700/25883], Loss: 2.3306, Perplexity: 10.2844\n",
      "Epoch [2/3], Step [10800/25883], Loss: 2.0684, Perplexity: 7.91191\n",
      "Epoch [2/3], Step [10900/25883], Loss: 2.2398, Perplexity: 9.39120\n",
      "Epoch [2/3], Step [11000/25883], Loss: 2.2862, Perplexity: 9.83717\n",
      "Epoch [2/3], Step [11100/25883], Loss: 2.3622, Perplexity: 10.6141\n",
      "Epoch [2/3], Step [11200/25883], Loss: 2.3196, Perplexity: 10.1717\n",
      "Epoch [2/3], Step [11300/25883], Loss: 2.7627, Perplexity: 15.8431\n",
      "Epoch [2/3], Step [11400/25883], Loss: 2.8027, Perplexity: 16.4892\n",
      "Epoch [2/3], Step [11500/25883], Loss: 2.3160, Perplexity: 10.1354\n",
      "Epoch [2/3], Step [11600/25883], Loss: 2.4462, Perplexity: 11.5444\n",
      "Epoch [2/3], Step [11700/25883], Loss: 2.6599, Perplexity: 14.2955\n",
      "Epoch [2/3], Step [11800/25883], Loss: 1.9797, Perplexity: 7.24042\n",
      "Epoch [2/3], Step [11900/25883], Loss: 1.9269, Perplexity: 6.86858\n",
      "Epoch [2/3], Step [12000/25883], Loss: 2.4539, Perplexity: 11.6334\n",
      "Epoch [2/3], Step [12100/25883], Loss: 1.8907, Perplexity: 6.62416\n",
      "Epoch [2/3], Step [12200/25883], Loss: 2.7728, Perplexity: 16.0029\n",
      "Epoch [2/3], Step [12300/25883], Loss: 2.7833, Perplexity: 16.1719\n",
      "Epoch [2/3], Step [12400/25883], Loss: 2.5809, Perplexity: 13.2094\n",
      "Epoch [2/3], Step [12500/25883], Loss: 2.2692, Perplexity: 9.67164\n",
      "Epoch [2/3], Step [12600/25883], Loss: 2.0131, Perplexity: 7.48664\n",
      "Epoch [2/3], Step [12700/25883], Loss: 2.1530, Perplexity: 8.61080\n",
      "Epoch [2/3], Step [12800/25883], Loss: 1.9703, Perplexity: 7.17308\n",
      "Epoch [2/3], Step [12900/25883], Loss: 1.9014, Perplexity: 6.69537\n",
      "Epoch [2/3], Step [13000/25883], Loss: 2.1922, Perplexity: 8.95534\n",
      "Epoch [2/3], Step [13100/25883], Loss: 2.1881, Perplexity: 8.91789\n",
      "Epoch [2/3], Step [13200/25883], Loss: 2.6151, Perplexity: 13.6689\n",
      "Epoch [2/3], Step [13300/25883], Loss: 2.6244, Perplexity: 13.7964\n",
      "Epoch [2/3], Step [13400/25883], Loss: 2.8719, Perplexity: 17.6705\n",
      "Epoch [2/3], Step [13500/25883], Loss: 2.8514, Perplexity: 17.3113\n",
      "Epoch [2/3], Step [13600/25883], Loss: 2.1178, Perplexity: 8.31253\n",
      "Epoch [2/3], Step [13700/25883], Loss: 1.9216, Perplexity: 6.83201\n",
      "Epoch [2/3], Step [13800/25883], Loss: 2.6484, Perplexity: 14.1318\n",
      "Epoch [2/3], Step [13900/25883], Loss: 2.6857, Perplexity: 14.6688\n",
      "Epoch [2/3], Step [14000/25883], Loss: 2.2138, Perplexity: 9.15016\n",
      "Epoch [2/3], Step [14100/25883], Loss: 2.6332, Perplexity: 13.9183\n",
      "Epoch [2/3], Step [14200/25883], Loss: 2.2112, Perplexity: 9.12688\n",
      "Epoch [2/3], Step [14300/25883], Loss: 2.6866, Perplexity: 14.6811\n",
      "Epoch [2/3], Step [14400/25883], Loss: 2.3983, Perplexity: 11.0046\n",
      "Epoch [2/3], Step [14500/25883], Loss: 1.8817, Perplexity: 6.56480\n",
      "Epoch [2/3], Step [14600/25883], Loss: 3.2215, Perplexity: 25.0653\n",
      "Epoch [2/3], Step [14700/25883], Loss: 2.1760, Perplexity: 8.81133\n",
      "Epoch [2/3], Step [14800/25883], Loss: 2.3586, Perplexity: 10.5762\n",
      "Epoch [2/3], Step [14900/25883], Loss: 2.2353, Perplexity: 9.34927\n",
      "Epoch [2/3], Step [15000/25883], Loss: 2.4934, Perplexity: 12.1018\n",
      "Epoch [2/3], Step [15100/25883], Loss: 1.8089, Perplexity: 6.10398\n",
      "Epoch [2/3], Step [15200/25883], Loss: 1.8911, Perplexity: 6.62690\n",
      "Epoch [2/3], Step [15300/25883], Loss: 2.2162, Perplexity: 9.17286\n",
      "Epoch [2/3], Step [15400/25883], Loss: 2.1713, Perplexity: 8.76990\n",
      "Epoch [2/3], Step [15500/25883], Loss: 2.1254, Perplexity: 8.37638\n",
      "Epoch [2/3], Step [15600/25883], Loss: 2.6480, Perplexity: 14.12545\n",
      "Epoch [2/3], Step [15700/25883], Loss: 2.3177, Perplexity: 10.1523\n",
      "Epoch [2/3], Step [15800/25883], Loss: 1.7843, Perplexity: 5.95536\n",
      "Epoch [2/3], Step [15900/25883], Loss: 2.5697, Perplexity: 13.0620\n",
      "Epoch [2/3], Step [16000/25883], Loss: 2.8685, Perplexity: 17.6109\n",
      "Epoch [2/3], Step [16100/25883], Loss: 2.4952, Perplexity: 12.1246\n",
      "Epoch [2/3], Step [16200/25883], Loss: 2.6421, Perplexity: 14.0430\n",
      "Epoch [2/3], Step [16300/25883], Loss: 2.3339, Perplexity: 10.3186\n",
      "Epoch [2/3], Step [16400/25883], Loss: 2.6342, Perplexity: 13.9317\n",
      "Epoch [2/3], Step [16500/25883], Loss: 2.2961, Perplexity: 9.93543\n",
      "Epoch [2/3], Step [16600/25883], Loss: 2.7576, Perplexity: 15.7617\n",
      "Epoch [2/3], Step [16700/25883], Loss: 2.5672, Perplexity: 13.0289\n",
      "Epoch [2/3], Step [16800/25883], Loss: 1.9462, Perplexity: 7.00199\n",
      "Epoch [2/3], Step [16900/25883], Loss: 2.2042, Perplexity: 9.06307\n",
      "Epoch [2/3], Step [17000/25883], Loss: 2.5840, Perplexity: 13.2497\n",
      "Epoch [2/3], Step [17100/25883], Loss: 2.2441, Perplexity: 9.43238\n",
      "Epoch [2/3], Step [17200/25883], Loss: 2.6208, Perplexity: 13.7470\n",
      "Epoch [2/3], Step [17300/25883], Loss: 2.8136, Perplexity: 16.6701\n",
      "Epoch [2/3], Step [17400/25883], Loss: 2.2824, Perplexity: 9.80036\n",
      "Epoch [2/3], Step [17500/25883], Loss: 2.1706, Perplexity: 8.76342\n",
      "Epoch [2/3], Step [17600/25883], Loss: 2.1493, Perplexity: 8.57895\n",
      "Epoch [2/3], Step [17700/25883], Loss: 2.3498, Perplexity: 10.4835\n",
      "Epoch [2/3], Step [17800/25883], Loss: 2.1191, Perplexity: 8.32399\n",
      "Epoch [2/3], Step [17900/25883], Loss: 2.6000, Perplexity: 13.4643\n",
      "Epoch [2/3], Step [18000/25883], Loss: 1.5499, Perplexity: 4.71090\n",
      "Epoch [2/3], Step [18100/25883], Loss: 2.4589, Perplexity: 11.6919\n",
      "Epoch [2/3], Step [18200/25883], Loss: 2.3428, Perplexity: 10.4104\n",
      "Epoch [2/3], Step [18300/25883], Loss: 2.2965, Perplexity: 9.93967\n",
      "Epoch [2/3], Step [18400/25883], Loss: 2.5655, Perplexity: 13.0066\n",
      "Epoch [2/3], Step [18500/25883], Loss: 2.0878, Perplexity: 8.06758\n",
      "Epoch [2/3], Step [18600/25883], Loss: 2.6568, Perplexity: 14.2506\n",
      "Epoch [2/3], Step [18700/25883], Loss: 2.4313, Perplexity: 11.3737\n",
      "Epoch [2/3], Step [18800/25883], Loss: 2.3890, Perplexity: 10.9031\n",
      "Epoch [2/3], Step [18900/25883], Loss: 2.2492, Perplexity: 9.47998\n",
      "Epoch [2/3], Step [19000/25883], Loss: 2.4131, Perplexity: 11.16908\n",
      "Epoch [2/3], Step [19100/25883], Loss: 2.2461, Perplexity: 9.45052\n",
      "Epoch [2/3], Step [19200/25883], Loss: 2.2152, Perplexity: 9.16322\n",
      "Epoch [2/3], Step [19300/25883], Loss: 2.2339, Perplexity: 9.33667\n",
      "Epoch [2/3], Step [19400/25883], Loss: 2.4051, Perplexity: 11.0794\n",
      "Epoch [2/3], Step [19500/25883], Loss: 2.5058, Perplexity: 12.2536\n",
      "Epoch [2/3], Step [19600/25883], Loss: 1.7828, Perplexity: 5.94638\n",
      "Epoch [2/3], Step [19700/25883], Loss: 2.2488, Perplexity: 9.47593\n",
      "Epoch [2/3], Step [19800/25883], Loss: 2.1664, Perplexity: 8.72653\n",
      "Epoch [2/3], Step [19900/25883], Loss: 2.1980, Perplexity: 9.00670\n",
      "Epoch [2/3], Step [20000/25883], Loss: 2.1037, Perplexity: 8.19643\n",
      "Epoch [2/3], Step [20100/25883], Loss: 2.2842, Perplexity: 9.81777\n",
      "Epoch [2/3], Step [20200/25883], Loss: 2.3050, Perplexity: 10.0245\n",
      "Epoch [2/3], Step [20300/25883], Loss: 2.3760, Perplexity: 10.7613\n",
      "Epoch [2/3], Step [20400/25883], Loss: 2.6432, Perplexity: 14.0578\n",
      "Epoch [2/3], Step [20500/25883], Loss: 2.6273, Perplexity: 13.8363\n",
      "Epoch [2/3], Step [20600/25883], Loss: 2.0913, Perplexity: 8.09546\n",
      "Epoch [2/3], Step [20700/25883], Loss: 2.1135, Perplexity: 8.27759\n",
      "Epoch [2/3], Step [20800/25883], Loss: 2.3886, Perplexity: 10.8979\n",
      "Epoch [2/3], Step [20900/25883], Loss: 2.0471, Perplexity: 7.74518\n",
      "Epoch [2/3], Step [21000/25883], Loss: 2.3435, Perplexity: 10.4173\n",
      "Epoch [2/3], Step [21100/25883], Loss: 2.4601, Perplexity: 11.7059\n",
      "Epoch [2/3], Step [21200/25883], Loss: 2.2079, Perplexity: 9.09688\n",
      "Epoch [2/3], Step [21300/25883], Loss: 2.0963, Perplexity: 8.13590\n",
      "Epoch [2/3], Step [21400/25883], Loss: 2.0997, Perplexity: 8.16362\n",
      "Epoch [2/3], Step [21500/25883], Loss: 2.4446, Perplexity: 11.5258\n",
      "Epoch [2/3], Step [21600/25883], Loss: 2.6045, Perplexity: 13.5250\n",
      "Epoch [2/3], Step [21700/25883], Loss: 2.3945, Perplexity: 10.9628\n",
      "Epoch [2/3], Step [21800/25883], Loss: 2.2882, Perplexity: 9.85707\n",
      "Epoch [2/3], Step [21900/25883], Loss: 2.1121, Perplexity: 8.26528\n",
      "Epoch [2/3], Step [22000/25883], Loss: 1.7146, Perplexity: 5.55462\n",
      "Epoch [2/3], Step [22100/25883], Loss: 2.4560, Perplexity: 11.6580\n",
      "Epoch [2/3], Step [22200/25883], Loss: 2.3513, Perplexity: 10.4993\n",
      "Epoch [2/3], Step [22300/25883], Loss: 2.2220, Perplexity: 9.22573\n",
      "Epoch [2/3], Step [22400/25883], Loss: 2.1457, Perplexity: 8.54783\n",
      "Epoch [2/3], Step [22500/25883], Loss: 2.1205, Perplexity: 8.335310\n",
      "Epoch [2/3], Step [22600/25883], Loss: 1.9894, Perplexity: 7.31100\n",
      "Epoch [2/3], Step [22700/25883], Loss: 2.8441, Perplexity: 17.1865\n",
      "Epoch [2/3], Step [22800/25883], Loss: 2.4071, Perplexity: 11.1017\n",
      "Epoch [2/3], Step [22900/25883], Loss: 2.5411, Perplexity: 12.6935\n",
      "Epoch [2/3], Step [23000/25883], Loss: 2.3703, Perplexity: 10.7009\n",
      "Epoch [2/3], Step [23100/25883], Loss: 2.0839, Perplexity: 8.03569\n",
      "Epoch [2/3], Step [23200/25883], Loss: 1.8730, Perplexity: 6.50806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [23300/25883], Loss: 2.2922, Perplexity: 9.89654\n",
      "Epoch [2/3], Step [23400/25883], Loss: 2.4566, Perplexity: 11.6647\n",
      "Epoch [2/3], Step [23500/25883], Loss: 2.2721, Perplexity: 9.70000\n",
      "Epoch [2/3], Step [23600/25883], Loss: 2.4656, Perplexity: 11.7708\n",
      "Epoch [2/3], Step [23700/25883], Loss: 2.4004, Perplexity: 11.0271\n",
      "Epoch [2/3], Step [23800/25883], Loss: 2.2418, Perplexity: 9.41063\n",
      "Epoch [2/3], Step [23900/25883], Loss: 2.4017, Perplexity: 11.0417\n",
      "Epoch [2/3], Step [24000/25883], Loss: 2.5185, Perplexity: 12.4105\n",
      "Epoch [2/3], Step [24100/25883], Loss: 2.7145, Perplexity: 15.0977\n",
      "Epoch [2/3], Step [24200/25883], Loss: 2.0945, Perplexity: 8.12172\n",
      "Epoch [2/3], Step [24300/25883], Loss: 2.0582, Perplexity: 7.83162\n",
      "Epoch [2/3], Step [24400/25883], Loss: 2.0833, Perplexity: 8.03083\n",
      "Epoch [2/3], Step [24500/25883], Loss: 2.0131, Perplexity: 7.48626\n",
      "Epoch [2/3], Step [24600/25883], Loss: 2.8842, Perplexity: 17.8900\n",
      "Epoch [2/3], Step [24700/25883], Loss: 2.5761, Perplexity: 13.1463\n",
      "Epoch [2/3], Step [24800/25883], Loss: 2.2460, Perplexity: 9.45027\n",
      "Epoch [2/3], Step [24900/25883], Loss: 2.3184, Perplexity: 10.1599\n",
      "Epoch [2/3], Step [25000/25883], Loss: 2.4650, Perplexity: 11.7640\n",
      "Epoch [2/3], Step [25100/25883], Loss: 2.1762, Perplexity: 8.81252\n",
      "Epoch [2/3], Step [25200/25883], Loss: 2.1340, Perplexity: 8.44823\n",
      "Epoch [2/3], Step [25300/25883], Loss: 2.3083, Perplexity: 10.0574\n",
      "Epoch [2/3], Step [25400/25883], Loss: 2.4209, Perplexity: 11.2562\n",
      "Epoch [2/3], Step [25500/25883], Loss: 2.3469, Perplexity: 10.4529\n",
      "Epoch [2/3], Step [25600/25883], Loss: 2.7436, Perplexity: 15.5431\n",
      "Epoch [2/3], Step [25700/25883], Loss: 2.4439, Perplexity: 11.5174\n",
      "Epoch [2/3], Step [25800/25883], Loss: 2.2967, Perplexity: 9.94182\n",
      "Epoch [3/3], Step [100/25883], Loss: 2.4530, Perplexity: 11.622939\n",
      "Epoch [3/3], Step [200/25883], Loss: 1.8275, Perplexity: 6.21851\n",
      "Epoch [3/3], Step [300/25883], Loss: 2.0606, Perplexity: 7.85058\n",
      "Epoch [3/3], Step [400/25883], Loss: 2.6367, Perplexity: 13.9668\n",
      "Epoch [3/3], Step [500/25883], Loss: 2.4898, Perplexity: 12.0584\n",
      "Epoch [3/3], Step [600/25883], Loss: 2.0411, Perplexity: 7.69875\n",
      "Epoch [3/3], Step [700/25883], Loss: 2.1730, Perplexity: 8.78455\n",
      "Epoch [3/3], Step [800/25883], Loss: 2.5102, Perplexity: 12.3070\n",
      "Epoch [3/3], Step [900/25883], Loss: 2.6889, Perplexity: 14.7150\n",
      "Epoch [3/3], Step [1000/25883], Loss: 2.3254, Perplexity: 10.2312\n",
      "Epoch [3/3], Step [1100/25883], Loss: 2.1837, Perplexity: 8.87893\n",
      "Epoch [3/3], Step [1200/25883], Loss: 2.6764, Perplexity: 14.5329\n",
      "Epoch [3/3], Step [1300/25883], Loss: 2.3449, Perplexity: 10.4326\n",
      "Epoch [3/3], Step [1400/25883], Loss: 2.6925, Perplexity: 14.7684\n",
      "Epoch [3/3], Step [1500/25883], Loss: 2.1788, Perplexity: 8.83575\n",
      "Epoch [3/3], Step [1600/25883], Loss: 2.2824, Perplexity: 9.80018\n",
      "Epoch [3/3], Step [1700/25883], Loss: 2.7735, Perplexity: 16.0144\n",
      "Epoch [3/3], Step [1800/25883], Loss: 2.5803, Perplexity: 13.2009\n",
      "Epoch [3/3], Step [1900/25883], Loss: 2.2824, Perplexity: 9.79984\n",
      "Epoch [3/3], Step [2000/25883], Loss: 1.9204, Perplexity: 6.82341\n",
      "Epoch [3/3], Step [2100/25883], Loss: 2.3381, Perplexity: 10.3616\n",
      "Epoch [3/3], Step [2200/25883], Loss: 1.9895, Perplexity: 7.31197\n",
      "Epoch [3/3], Step [2300/25883], Loss: 2.0446, Perplexity: 7.72601\n",
      "Epoch [3/3], Step [2400/25883], Loss: 2.0699, Perplexity: 7.92387\n",
      "Epoch [3/3], Step [2500/25883], Loss: 3.0327, Perplexity: 20.7527\n",
      "Epoch [3/3], Step [2600/25883], Loss: 2.3064, Perplexity: 10.0384\n",
      "Epoch [3/3], Step [2700/25883], Loss: 2.3892, Perplexity: 10.9052\n",
      "Epoch [3/3], Step [2800/25883], Loss: 2.3458, Perplexity: 10.4418\n",
      "Epoch [3/3], Step [2900/25883], Loss: 2.0829, Perplexity: 8.02765\n",
      "Epoch [3/3], Step [3000/25883], Loss: 2.3828, Perplexity: 10.8351\n",
      "Epoch [3/3], Step [3100/25883], Loss: 2.5714, Perplexity: 13.0846\n",
      "Epoch [3/3], Step [3200/25883], Loss: 2.1774, Perplexity: 8.82306\n",
      "Epoch [3/3], Step [3300/25883], Loss: 2.6186, Perplexity: 13.7166\n",
      "Epoch [3/3], Step [3400/25883], Loss: 2.1341, Perplexity: 8.44946\n",
      "Epoch [3/3], Step [3500/25883], Loss: 2.4433, Perplexity: 11.5112\n",
      "Epoch [3/3], Step [3600/25883], Loss: 1.9862, Perplexity: 7.28764\n",
      "Epoch [3/3], Step [3700/25883], Loss: 3.1580, Perplexity: 23.5236\n",
      "Epoch [3/3], Step [3800/25883], Loss: 2.3456, Perplexity: 10.4395\n",
      "Epoch [3/3], Step [3900/25883], Loss: 2.0014, Perplexity: 7.39937\n",
      "Epoch [3/3], Step [4000/25883], Loss: 2.4809, Perplexity: 11.9514\n",
      "Epoch [3/3], Step [4100/25883], Loss: 2.3474, Perplexity: 10.4586\n",
      "Epoch [3/3], Step [4200/25883], Loss: 2.6510, Perplexity: 14.1675\n",
      "Epoch [3/3], Step [4300/25883], Loss: 1.9618, Perplexity: 7.11228\n",
      "Epoch [3/3], Step [4400/25883], Loss: 2.8772, Perplexity: 17.7645\n",
      "Epoch [3/3], Step [4500/25883], Loss: 2.4949, Perplexity: 12.1202\n",
      "Epoch [3/3], Step [4600/25883], Loss: 2.3113, Perplexity: 10.0880\n",
      "Epoch [3/3], Step [4700/25883], Loss: 2.1074, Perplexity: 8.22648\n",
      "Epoch [3/3], Step [4800/25883], Loss: 2.5784, Perplexity: 13.1754\n",
      "Epoch [3/3], Step [4900/25883], Loss: 2.5380, Perplexity: 12.6539\n",
      "Epoch [3/3], Step [5000/25883], Loss: 2.6514, Perplexity: 14.1739\n",
      "Epoch [3/3], Step [5100/25883], Loss: 1.9000, Perplexity: 6.68583\n",
      "Epoch [3/3], Step [5200/25883], Loss: 1.7786, Perplexity: 5.92174\n",
      "Epoch [3/3], Step [5300/25883], Loss: 2.2473, Perplexity: 9.46241\n",
      "Epoch [3/3], Step [5400/25883], Loss: 2.0873, Perplexity: 8.06308\n",
      "Epoch [3/3], Step [5500/25883], Loss: 2.1561, Perplexity: 8.63711\n",
      "Epoch [3/3], Step [5600/25883], Loss: 2.2549, Perplexity: 9.53422\n",
      "Epoch [3/3], Step [5700/25883], Loss: 2.0017, Perplexity: 7.40198\n",
      "Epoch [3/3], Step [5800/25883], Loss: 1.9954, Perplexity: 7.35547\n",
      "Epoch [3/3], Step [5900/25883], Loss: 2.6508, Perplexity: 14.1648\n",
      "Epoch [3/3], Step [6000/25883], Loss: 2.3228, Perplexity: 10.2041\n",
      "Epoch [3/3], Step [6100/25883], Loss: 2.4335, Perplexity: 11.3989\n",
      "Epoch [3/3], Step [6200/25883], Loss: 2.1717, Perplexity: 8.77305\n",
      "Epoch [3/3], Step [6300/25883], Loss: 2.3481, Perplexity: 10.4654\n",
      "Epoch [3/3], Step [6400/25883], Loss: 2.6700, Perplexity: 14.4407\n",
      "Epoch [3/3], Step [6500/25883], Loss: 2.2626, Perplexity: 9.60813\n",
      "Epoch [3/3], Step [6600/25883], Loss: 2.5065, Perplexity: 12.2619\n",
      "Epoch [3/3], Step [6700/25883], Loss: 2.3355, Perplexity: 10.3347\n",
      "Epoch [3/3], Step [6800/25883], Loss: 2.3651, Perplexity: 10.6448\n",
      "Epoch [3/3], Step [6900/25883], Loss: 2.3084, Perplexity: 10.0585\n",
      "Epoch [3/3], Step [7000/25883], Loss: 2.3013, Perplexity: 9.98741\n",
      "Epoch [3/3], Step [7100/25883], Loss: 2.1915, Perplexity: 8.94856\n",
      "Epoch [3/3], Step [7200/25883], Loss: 2.5673, Perplexity: 13.0300\n",
      "Epoch [3/3], Step [7300/25883], Loss: 2.6430, Perplexity: 14.0550\n",
      "Epoch [3/3], Step [7400/25883], Loss: 2.0788, Perplexity: 7.99511\n",
      "Epoch [3/3], Step [7500/25883], Loss: 3.0251, Perplexity: 20.5968\n",
      "Epoch [3/3], Step [7600/25883], Loss: 2.4311, Perplexity: 11.3713\n",
      "Epoch [3/3], Step [7700/25883], Loss: 2.4085, Perplexity: 11.1168\n",
      "Epoch [3/3], Step [7800/25883], Loss: 2.4510, Perplexity: 11.5998\n",
      "Epoch [3/3], Step [7900/25883], Loss: 2.1571, Perplexity: 8.64565\n",
      "Epoch [3/3], Step [8000/25883], Loss: 2.2049, Perplexity: 9.06910\n",
      "Epoch [3/3], Step [8100/25883], Loss: 2.4729, Perplexity: 11.8566\n",
      "Epoch [3/3], Step [8200/25883], Loss: 2.1958, Perplexity: 8.98747\n",
      "Epoch [3/3], Step [8300/25883], Loss: 2.2915, Perplexity: 9.89037\n",
      "Epoch [3/3], Step [8400/25883], Loss: 2.2108, Perplexity: 9.12274\n",
      "Epoch [3/3], Step [8500/25883], Loss: 2.1814, Perplexity: 8.85907\n",
      "Epoch [3/3], Step [8600/25883], Loss: 2.2523, Perplexity: 9.50936\n",
      "Epoch [3/3], Step [8700/25883], Loss: 2.4982, Perplexity: 12.1608\n",
      "Epoch [3/3], Step [8800/25883], Loss: 2.2981, Perplexity: 9.95508\n",
      "Epoch [3/3], Step [8900/25883], Loss: 1.8729, Perplexity: 6.50681\n",
      "Epoch [3/3], Step [9000/25883], Loss: 2.1781, Perplexity: 8.82980\n",
      "Epoch [3/3], Step [9100/25883], Loss: 1.9339, Perplexity: 6.91613\n",
      "Epoch [3/3], Step [9200/25883], Loss: 2.2382, Perplexity: 9.37689\n",
      "Epoch [3/3], Step [9300/25883], Loss: 2.6464, Perplexity: 14.1028\n",
      "Epoch [3/3], Step [9400/25883], Loss: 2.1590, Perplexity: 8.66253\n",
      "Epoch [3/3], Step [9500/25883], Loss: 2.5788, Perplexity: 13.1807\n",
      "Epoch [3/3], Step [9600/25883], Loss: 2.2889, Perplexity: 9.86368\n",
      "Epoch [3/3], Step [9700/25883], Loss: 2.6844, Perplexity: 14.6496\n",
      "Epoch [3/3], Step [9800/25883], Loss: 2.0380, Perplexity: 7.67526\n",
      "Epoch [3/3], Step [9900/25883], Loss: 2.2475, Perplexity: 9.46452\n",
      "Epoch [3/3], Step [10000/25883], Loss: 2.1093, Perplexity: 8.2426\n",
      "Epoch [3/3], Step [10100/25883], Loss: 2.5160, Perplexity: 12.3794\n",
      "Epoch [3/3], Step [10200/25883], Loss: 2.2616, Perplexity: 9.59830\n",
      "Epoch [3/3], Step [10300/25883], Loss: 2.1174, Perplexity: 8.30977\n",
      "Epoch [3/3], Step [10400/25883], Loss: 2.2979, Perplexity: 9.95313\n",
      "Epoch [3/3], Step [10500/25883], Loss: 1.9669, Perplexity: 7.14822\n",
      "Epoch [3/3], Step [10600/25883], Loss: 2.1966, Perplexity: 8.99470\n",
      "Epoch [3/3], Step [10700/25883], Loss: 2.3027, Perplexity: 10.0007\n",
      "Epoch [3/3], Step [10800/25883], Loss: 2.0416, Perplexity: 7.70280\n",
      "Epoch [3/3], Step [10900/25883], Loss: 2.2760, Perplexity: 9.73788\n",
      "Epoch [3/3], Step [11000/25883], Loss: 2.3462, Perplexity: 10.4456\n",
      "Epoch [3/3], Step [11100/25883], Loss: 2.6559, Perplexity: 14.2382\n",
      "Epoch [3/3], Step [11200/25883], Loss: 2.3676, Perplexity: 10.6715\n",
      "Epoch [3/3], Step [11300/25883], Loss: 2.2006, Perplexity: 9.03019\n",
      "Epoch [3/3], Step [11400/25883], Loss: 2.2719, Perplexity: 9.69820\n",
      "Epoch [3/3], Step [11500/25883], Loss: 2.3342, Perplexity: 10.3212\n",
      "Epoch [3/3], Step [11600/25883], Loss: 2.2885, Perplexity: 9.85983\n",
      "Epoch [3/3], Step [11700/25883], Loss: 2.3208, Perplexity: 10.1842\n",
      "Epoch [3/3], Step [11800/25883], Loss: 2.2821, Perplexity: 9.79689\n",
      "Epoch [3/3], Step [11900/25883], Loss: 2.5315, Perplexity: 12.5722\n",
      "Epoch [3/3], Step [12000/25883], Loss: 2.2565, Perplexity: 9.54939\n",
      "Epoch [3/3], Step [12100/25883], Loss: 2.2155, Perplexity: 9.16594\n",
      "Epoch [3/3], Step [12200/25883], Loss: 2.3028, Perplexity: 10.0022\n",
      "Epoch [3/3], Step [12300/25883], Loss: 2.6151, Perplexity: 13.6691\n",
      "Epoch [3/3], Step [12400/25883], Loss: 2.2785, Perplexity: 9.76176\n",
      "Epoch [3/3], Step [12500/25883], Loss: 2.4488, Perplexity: 11.5741\n",
      "Epoch [3/3], Step [12600/25883], Loss: 2.4774, Perplexity: 11.9099\n",
      "Epoch [3/3], Step [12700/25883], Loss: 2.7466, Perplexity: 15.5899\n",
      "Epoch [3/3], Step [12800/25883], Loss: 2.4614, Perplexity: 11.7217\n",
      "Epoch [3/3], Step [12900/25883], Loss: 1.9018, Perplexity: 6.69807\n",
      "Epoch [3/3], Step [13000/25883], Loss: 2.1107, Perplexity: 8.25438\n",
      "Epoch [3/3], Step [13100/25883], Loss: 1.9751, Perplexity: 7.20734\n",
      "Epoch [3/3], Step [13200/25883], Loss: 1.9976, Perplexity: 7.37148\n",
      "Epoch [3/3], Step [13300/25883], Loss: 2.2093, Perplexity: 9.10943\n",
      "Epoch [3/3], Step [13400/25883], Loss: 2.2718, Perplexity: 9.69682\n",
      "Epoch [3/3], Step [13500/25883], Loss: 2.1418, Perplexity: 8.51493\n",
      "Epoch [3/3], Step [13600/25883], Loss: 2.0710, Perplexity: 7.93256\n",
      "Epoch [3/3], Step [13700/25883], Loss: 2.0410, Perplexity: 7.69830\n",
      "Epoch [3/3], Step [13800/25883], Loss: 2.0008, Perplexity: 7.39528\n",
      "Epoch [3/3], Step [13900/25883], Loss: 1.9305, Perplexity: 6.892812\n",
      "Epoch [3/3], Step [14000/25883], Loss: 2.4853, Perplexity: 12.0052\n",
      "Epoch [3/3], Step [14100/25883], Loss: 2.8237, Perplexity: 16.8387\n",
      "Epoch [3/3], Step [14200/25883], Loss: 2.3575, Perplexity: 10.5648\n",
      "Epoch [3/3], Step [14300/25883], Loss: 1.9483, Perplexity: 7.01669\n",
      "Epoch [3/3], Step [14400/25883], Loss: 2.3518, Perplexity: 10.5048\n",
      "Epoch [3/3], Step [14500/25883], Loss: 2.3104, Perplexity: 10.0787\n",
      "Epoch [3/3], Step [14600/25883], Loss: 2.3545, Perplexity: 10.5333\n",
      "Epoch [3/3], Step [14700/25883], Loss: 2.1510, Perplexity: 8.59365\n",
      "Epoch [3/3], Step [14800/25883], Loss: 2.3739, Perplexity: 10.73932\n",
      "Epoch [3/3], Step [14900/25883], Loss: 2.5959, Perplexity: 13.4091\n",
      "Epoch [3/3], Step [15000/25883], Loss: 2.2155, Perplexity: 9.16634\n",
      "Epoch [3/3], Step [15100/25883], Loss: 2.2799, Perplexity: 9.77607\n",
      "Epoch [3/3], Step [15200/25883], Loss: 2.0663, Perplexity: 7.89578\n",
      "Epoch [3/3], Step [15300/25883], Loss: 2.5259, Perplexity: 12.5027\n",
      "Epoch [3/3], Step [15400/25883], Loss: 2.3621, Perplexity: 10.6133\n",
      "Epoch [3/3], Step [15500/25883], Loss: 1.8003, Perplexity: 6.05175\n",
      "Epoch [3/3], Step [15600/25883], Loss: 2.1026, Perplexity: 8.18787\n",
      "Epoch [3/3], Step [15700/25883], Loss: 2.2653, Perplexity: 9.63389\n",
      "Epoch [3/3], Step [15800/25883], Loss: 2.3489, Perplexity: 10.4740\n",
      "Epoch [3/3], Step [15900/25883], Loss: 2.1357, Perplexity: 8.463089\n",
      "Epoch [3/3], Step [16000/25883], Loss: 2.0429, Perplexity: 7.71302\n",
      "Epoch [3/3], Step [16100/25883], Loss: 2.5854, Perplexity: 13.2686\n",
      "Epoch [3/3], Step [16200/25883], Loss: 2.0333, Perplexity: 7.63904\n",
      "Epoch [3/3], Step [16300/25883], Loss: 2.0248, Perplexity: 7.57464\n",
      "Epoch [3/3], Step [16400/25883], Loss: 2.5369, Perplexity: 12.6410\n",
      "Epoch [3/3], Step [16500/25883], Loss: 2.1061, Perplexity: 8.21644\n",
      "Epoch [3/3], Step [16600/25883], Loss: 2.6008, Perplexity: 13.4751\n",
      "Epoch [3/3], Step [16700/25883], Loss: 2.2706, Perplexity: 9.68541\n",
      "Epoch [3/3], Step [16800/25883], Loss: 2.4256, Perplexity: 11.3096\n",
      "Epoch [3/3], Step [16900/25883], Loss: 2.4046, Perplexity: 11.0744\n",
      "Epoch [3/3], Step [17000/25883], Loss: 2.1844, Perplexity: 8.88529\n",
      "Epoch [3/3], Step [17100/25883], Loss: 2.5154, Perplexity: 12.3718\n",
      "Epoch [3/3], Step [17200/25883], Loss: 2.5655, Perplexity: 13.0070\n",
      "Epoch [3/3], Step [17300/25883], Loss: 2.0157, Perplexity: 7.50609\n",
      "Epoch [3/3], Step [17400/25883], Loss: 2.0471, Perplexity: 7.74575\n",
      "Epoch [3/3], Step [17500/25883], Loss: 2.3077, Perplexity: 10.0509\n",
      "Epoch [3/3], Step [17600/25883], Loss: 2.4779, Perplexity: 11.9157\n",
      "Epoch [3/3], Step [17700/25883], Loss: 2.1373, Perplexity: 8.47627\n",
      "Epoch [3/3], Step [17800/25883], Loss: 2.0496, Perplexity: 7.76454\n",
      "Epoch [3/3], Step [17900/25883], Loss: 3.0605, Perplexity: 21.3391\n",
      "Epoch [3/3], Step [18000/25883], Loss: 3.7543, Perplexity: 42.7034\n",
      "Epoch [3/3], Step [18100/25883], Loss: 2.3022, Perplexity: 9.99645\n",
      "Epoch [3/3], Step [18200/25883], Loss: 2.2131, Perplexity: 9.14368\n",
      "Epoch [3/3], Step [18300/25883], Loss: 2.3161, Perplexity: 10.1360\n",
      "Epoch [3/3], Step [18400/25883], Loss: 2.2846, Perplexity: 9.82143\n",
      "Epoch [3/3], Step [18500/25883], Loss: 2.1690, Perplexity: 8.74954\n",
      "Epoch [3/3], Step [18600/25883], Loss: 2.1222, Perplexity: 8.34922\n",
      "Epoch [3/3], Step [18700/25883], Loss: 1.9248, Perplexity: 6.85416\n",
      "Epoch [3/3], Step [18800/25883], Loss: 2.3483, Perplexity: 10.4680\n",
      "Epoch [3/3], Step [18900/25883], Loss: 2.0085, Perplexity: 7.45204\n",
      "Epoch [3/3], Step [19000/25883], Loss: 2.3847, Perplexity: 10.8561\n",
      "Epoch [3/3], Step [19100/25883], Loss: 1.7336, Perplexity: 5.66119\n",
      "Epoch [3/3], Step [19200/25883], Loss: 2.1502, Perplexity: 8.58682\n",
      "Epoch [3/3], Step [19300/25883], Loss: 1.8504, Perplexity: 6.36218\n",
      "Epoch [3/3], Step [19400/25883], Loss: 2.4429, Perplexity: 11.5062\n",
      "Epoch [3/3], Step [19500/25883], Loss: 2.3408, Perplexity: 10.3900\n",
      "Epoch [3/3], Step [19600/25883], Loss: 2.5960, Perplexity: 13.40947\n",
      "Epoch [3/3], Step [19700/25883], Loss: 2.1997, Perplexity: 9.02257\n",
      "Epoch [3/3], Step [19800/25883], Loss: 2.4067, Perplexity: 11.0967\n",
      "Epoch [3/3], Step [19900/25883], Loss: 2.2859, Perplexity: 9.83420\n",
      "Epoch [3/3], Step [20000/25883], Loss: 2.0562, Perplexity: 7.81594\n",
      "Epoch [3/3], Step [20100/25883], Loss: 2.3384, Perplexity: 10.3651\n",
      "Epoch [3/3], Step [20200/25883], Loss: 1.8029, Perplexity: 6.06728\n",
      "Epoch [3/3], Step [20300/25883], Loss: 3.1669, Perplexity: 23.7329\n",
      "Epoch [3/3], Step [20400/25883], Loss: 2.4030, Perplexity: 11.0567\n",
      "Epoch [3/3], Step [20500/25883], Loss: 2.0284, Perplexity: 7.60154\n",
      "Epoch [3/3], Step [20600/25883], Loss: 2.6601, Perplexity: 14.2982\n",
      "Epoch [3/3], Step [20700/25883], Loss: 2.5427, Perplexity: 12.7136\n",
      "Epoch [3/3], Step [20800/25883], Loss: 2.4322, Perplexity: 11.3841\n",
      "Epoch [3/3], Step [20900/25883], Loss: 2.2932, Perplexity: 9.90636\n",
      "Epoch [3/3], Step [21000/25883], Loss: 2.5778, Perplexity: 13.1686\n",
      "Epoch [3/3], Step [21100/25883], Loss: 2.0577, Perplexity: 7.82808\n",
      "Epoch [3/3], Step [21200/25883], Loss: 2.1366, Perplexity: 8.47084\n",
      "Epoch [3/3], Step [21300/25883], Loss: 2.0183, Perplexity: 7.52562\n",
      "Epoch [3/3], Step [21400/25883], Loss: 2.3282, Perplexity: 10.2595\n",
      "Epoch [3/3], Step [21500/25883], Loss: 2.3462, Perplexity: 10.4454\n",
      "Epoch [3/3], Step [21600/25883], Loss: 2.5839, Perplexity: 13.2489\n",
      "Epoch [3/3], Step [21700/25883], Loss: 1.9541, Perplexity: 7.05777\n",
      "Epoch [3/3], Step [21800/25883], Loss: 2.3583, Perplexity: 10.5734\n",
      "Epoch [3/3], Step [21900/25883], Loss: 2.2142, Perplexity: 9.15451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [22000/25883], Loss: 2.0684, Perplexity: 7.91211\n",
      "Epoch [3/3], Step [22100/25883], Loss: 2.4532, Perplexity: 11.6257\n",
      "Epoch [3/3], Step [22200/25883], Loss: 2.4210, Perplexity: 11.2574\n",
      "Epoch [3/3], Step [22300/25883], Loss: 1.9320, Perplexity: 6.90349\n",
      "Epoch [3/3], Step [22400/25883], Loss: 2.0477, Perplexity: 7.74975\n",
      "Epoch [3/3], Step [22500/25883], Loss: 2.7386, Perplexity: 15.4657\n",
      "Epoch [3/3], Step [22600/25883], Loss: 2.0973, Perplexity: 8.14439\n",
      "Epoch [3/3], Step [22700/25883], Loss: 3.0162, Perplexity: 20.4140\n",
      "Epoch [3/3], Step [22800/25883], Loss: 1.6844, Perplexity: 5.38922\n",
      "Epoch [3/3], Step [22900/25883], Loss: 2.1403, Perplexity: 8.50180\n",
      "Epoch [3/3], Step [23000/25883], Loss: 2.4785, Perplexity: 11.9239\n",
      "Epoch [3/3], Step [23100/25883], Loss: 2.0092, Perplexity: 7.45724\n",
      "Epoch [3/3], Step [23200/25883], Loss: 2.3207, Perplexity: 10.1823\n",
      "Epoch [3/3], Step [23300/25883], Loss: 2.1566, Perplexity: 8.64132\n",
      "Epoch [3/3], Step [23400/25883], Loss: 2.5844, Perplexity: 13.2555\n",
      "Epoch [3/3], Step [23500/25883], Loss: 2.2327, Perplexity: 9.325063\n",
      "Epoch [3/3], Step [23600/25883], Loss: 2.4284, Perplexity: 11.3405\n",
      "Epoch [3/3], Step [23700/25883], Loss: 2.1505, Perplexity: 8.58953\n",
      "Epoch [3/3], Step [23800/25883], Loss: 2.2209, Perplexity: 9.21564\n",
      "Epoch [3/3], Step [23900/25883], Loss: 2.1070, Perplexity: 8.22383\n",
      "Epoch [3/3], Step [24000/25883], Loss: 1.8962, Perplexity: 6.660704\n",
      "Epoch [3/3], Step [24100/25883], Loss: 2.5658, Perplexity: 13.0105\n",
      "Epoch [3/3], Step [24200/25883], Loss: 2.2218, Perplexity: 9.22397\n",
      "Epoch [3/3], Step [24300/25883], Loss: 2.2247, Perplexity: 9.25040\n",
      "Epoch [3/3], Step [24400/25883], Loss: 2.6173, Perplexity: 13.6992\n",
      "Epoch [3/3], Step [24500/25883], Loss: 2.4799, Perplexity: 11.9400\n",
      "Epoch [3/3], Step [24600/25883], Loss: 2.0488, Perplexity: 7.75840\n",
      "Epoch [3/3], Step [24700/25883], Loss: 2.2756, Perplexity: 9.73397\n",
      "Epoch [3/3], Step [24800/25883], Loss: 2.1078, Perplexity: 8.23024\n",
      "Epoch [3/3], Step [24900/25883], Loss: 2.2104, Perplexity: 9.11912\n",
      "Epoch [3/3], Step [25000/25883], Loss: 1.9700, Perplexity: 7.17072\n",
      "Epoch [3/3], Step [25100/25883], Loss: 1.9484, Perplexity: 7.01766\n",
      "Epoch [3/3], Step [25200/25883], Loss: 2.1226, Perplexity: 8.35259\n",
      "Epoch [3/3], Step [25300/25883], Loss: 2.4552, Perplexity: 11.6492\n",
      "Epoch [3/3], Step [25400/25883], Loss: 1.9729, Perplexity: 7.19175\n",
      "Epoch [3/3], Step [25500/25883], Loss: 2.2869, Perplexity: 9.84444\n",
      "Epoch [3/3], Step [25600/25883], Loss: 1.9874, Perplexity: 7.29663\n",
      "Epoch [3/3], Step [25700/25883], Loss: 2.3148, Perplexity: 10.1227\n",
      "Epoch [3/3], Step [25800/25883], Loss: 2.1745, Perplexity: 8.79759\n",
      "Epoch [3/3], Step [25883/25883], Loss: 2.2514, Perplexity: 9.50120"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
